{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'dtangle' was built under R version 4.0.5\"\n"
     ]
    }
   ],
   "source": [
    "library('dtangle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = shen_orr_ex$data$log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>42</li><li>600</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 42\n",
       "\\item 600\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 42\n",
       "2. 600\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]  42 600"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 600</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>X1367566_at</th><th scope=col>X1367568_a_at</th><th scope=col>X1367570_at</th><th scope=col>X1367584_at</th><th scope=col>X1367614_at</th><th scope=col>X1367647_at</th><th scope=col>X1367661_at</th><th scope=col>X1367664_at</th><th scope=col>X1367804_at</th><th scope=col>X1367816_at</th><th scope=col>...</th><th scope=col>X1398282_at</th><th scope=col>X1398318_at</th><th scope=col>X1398357_at</th><th scope=col>X1398368_at</th><th scope=col>X1398514_at</th><th scope=col>X1398577_at</th><th scope=col>X1398578_at</th><th scope=col>X1398625_at</th><th scope=col>X1398634_at</th><th scope=col>X1398716_at</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>...</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>GSM495209</th><td>3.396192</td><td>7.685769</td><td>5.722330</td><td>6.628653</td><td>6.659618</td><td>12.737089</td><td>5.163315</td><td>3.101658</td><td>11.459725</td><td>4.819886</td><td>...</td><td>10.524515</td><td>4.401075</td><td> 3.327705</td><td>4.043979</td><td>10.773481</td><td>12.232607</td><td> 3.410767</td><td> 2.587550</td><td>10.230386</td><td>2.275608</td></tr>\n",
       "\t<tr><th scope=row>GSM495210</th><td>2.882626</td><td>7.759002</td><td>6.005583</td><td>6.771917</td><td>6.655919</td><td>12.755694</td><td>5.491298</td><td>3.095527</td><td>11.439763</td><td>4.708422</td><td>...</td><td>10.548672</td><td>3.953334</td><td> 3.605773</td><td>3.779552</td><td>10.819626</td><td>12.097947</td><td> 3.605773</td><td> 2.429901</td><td>10.233172</td><td>1.937317</td></tr>\n",
       "\t<tr><th scope=row>GSM495211</th><td>3.072980</td><td>7.598871</td><td>5.741630</td><td>6.564820</td><td>6.741483</td><td>12.725779</td><td>5.291097</td><td>2.864352</td><td>11.425470</td><td>4.753807</td><td>...</td><td>10.536997</td><td>4.766228</td><td> 3.657362</td><td>4.435827</td><td>10.861409</td><td>12.218917</td><td> 3.309184</td><td> 2.611046</td><td>10.202474</td><td>1.919408</td></tr>\n",
       "\t<tr><th scope=row>GSM495212</th><td>3.168440</td><td>7.209959</td><td>6.396841</td><td>7.040779</td><td>5.282323</td><td> 6.056859</td><td>4.714119</td><td>2.349998</td><td> 4.009121</td><td>8.512256</td><td>...</td><td> 3.577845</td><td>3.825272</td><td>11.283218</td><td>9.241972</td><td> 2.816389</td><td> 4.849044</td><td>10.391058</td><td>10.849582</td><td> 2.787449</td><td>7.923482</td></tr>\n",
       "\t<tr><th scope=row>GSM495213</th><td>3.087147</td><td>7.321854</td><td>6.483467</td><td>7.054886</td><td>5.469455</td><td> 6.045071</td><td>4.713800</td><td>2.570193</td><td> 4.100364</td><td>8.640070</td><td>...</td><td> 3.529783</td><td>3.634708</td><td>11.322901</td><td>9.206841</td><td> 2.836462</td><td> 4.920250</td><td>10.263996</td><td>10.778257</td><td> 2.581674</td><td>8.127655</td></tr>\n",
       "\t<tr><th scope=row>GSM495214</th><td>2.768613</td><td>7.277230</td><td>6.747241</td><td>7.079465</td><td>5.277774</td><td> 5.821334</td><td>4.967451</td><td>2.819242</td><td> 4.110803</td><td>8.554801</td><td>...</td><td> 3.700635</td><td>3.726439</td><td>11.288997</td><td>9.229028</td><td> 2.599532</td><td> 5.177272</td><td>10.423879</td><td>10.999074</td><td> 2.230049</td><td>8.029409</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 600\n",
       "\\begin{tabular}{r|lllllllllllllllllllll}\n",
       "  & X1367566\\_at & X1367568\\_a\\_at & X1367570\\_at & X1367584\\_at & X1367614\\_at & X1367647\\_at & X1367661\\_at & X1367664\\_at & X1367804\\_at & X1367816\\_at & ... & X1398282\\_at & X1398318\\_at & X1398357\\_at & X1398368\\_at & X1398514\\_at & X1398577\\_at & X1398578\\_at & X1398625\\_at & X1398634\\_at & X1398716\\_at\\\\\n",
       "  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & ... & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\tGSM495209 & 3.396192 & 7.685769 & 5.722330 & 6.628653 & 6.659618 & 12.737089 & 5.163315 & 3.101658 & 11.459725 & 4.819886 & ... & 10.524515 & 4.401075 &  3.327705 & 4.043979 & 10.773481 & 12.232607 &  3.410767 &  2.587550 & 10.230386 & 2.275608\\\\\n",
       "\tGSM495210 & 2.882626 & 7.759002 & 6.005583 & 6.771917 & 6.655919 & 12.755694 & 5.491298 & 3.095527 & 11.439763 & 4.708422 & ... & 10.548672 & 3.953334 &  3.605773 & 3.779552 & 10.819626 & 12.097947 &  3.605773 &  2.429901 & 10.233172 & 1.937317\\\\\n",
       "\tGSM495211 & 3.072980 & 7.598871 & 5.741630 & 6.564820 & 6.741483 & 12.725779 & 5.291097 & 2.864352 & 11.425470 & 4.753807 & ... & 10.536997 & 4.766228 &  3.657362 & 4.435827 & 10.861409 & 12.218917 &  3.309184 &  2.611046 & 10.202474 & 1.919408\\\\\n",
       "\tGSM495212 & 3.168440 & 7.209959 & 6.396841 & 7.040779 & 5.282323 &  6.056859 & 4.714119 & 2.349998 &  4.009121 & 8.512256 & ... &  3.577845 & 3.825272 & 11.283218 & 9.241972 &  2.816389 &  4.849044 & 10.391058 & 10.849582 &  2.787449 & 7.923482\\\\\n",
       "\tGSM495213 & 3.087147 & 7.321854 & 6.483467 & 7.054886 & 5.469455 &  6.045071 & 4.713800 & 2.570193 &  4.100364 & 8.640070 & ... &  3.529783 & 3.634708 & 11.322901 & 9.206841 &  2.836462 &  4.920250 & 10.263996 & 10.778257 &  2.581674 & 8.127655\\\\\n",
       "\tGSM495214 & 2.768613 & 7.277230 & 6.747241 & 7.079465 & 5.277774 &  5.821334 & 4.967451 & 2.819242 &  4.110803 & 8.554801 & ... &  3.700635 & 3.726439 & 11.288997 & 9.229028 &  2.599532 &  5.177272 & 10.423879 & 10.999074 &  2.230049 & 8.029409\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 600\n",
       "\n",
       "| <!--/--> | X1367566_at &lt;dbl&gt; | X1367568_a_at &lt;dbl&gt; | X1367570_at &lt;dbl&gt; | X1367584_at &lt;dbl&gt; | X1367614_at &lt;dbl&gt; | X1367647_at &lt;dbl&gt; | X1367661_at &lt;dbl&gt; | X1367664_at &lt;dbl&gt; | X1367804_at &lt;dbl&gt; | X1367816_at &lt;dbl&gt; | ... ... | X1398282_at &lt;dbl&gt; | X1398318_at &lt;dbl&gt; | X1398357_at &lt;dbl&gt; | X1398368_at &lt;dbl&gt; | X1398514_at &lt;dbl&gt; | X1398577_at &lt;dbl&gt; | X1398578_at &lt;dbl&gt; | X1398625_at &lt;dbl&gt; | X1398634_at &lt;dbl&gt; | X1398716_at &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| GSM495209 | 3.396192 | 7.685769 | 5.722330 | 6.628653 | 6.659618 | 12.737089 | 5.163315 | 3.101658 | 11.459725 | 4.819886 | ... | 10.524515 | 4.401075 |  3.327705 | 4.043979 | 10.773481 | 12.232607 |  3.410767 |  2.587550 | 10.230386 | 2.275608 |\n",
       "| GSM495210 | 2.882626 | 7.759002 | 6.005583 | 6.771917 | 6.655919 | 12.755694 | 5.491298 | 3.095527 | 11.439763 | 4.708422 | ... | 10.548672 | 3.953334 |  3.605773 | 3.779552 | 10.819626 | 12.097947 |  3.605773 |  2.429901 | 10.233172 | 1.937317 |\n",
       "| GSM495211 | 3.072980 | 7.598871 | 5.741630 | 6.564820 | 6.741483 | 12.725779 | 5.291097 | 2.864352 | 11.425470 | 4.753807 | ... | 10.536997 | 4.766228 |  3.657362 | 4.435827 | 10.861409 | 12.218917 |  3.309184 |  2.611046 | 10.202474 | 1.919408 |\n",
       "| GSM495212 | 3.168440 | 7.209959 | 6.396841 | 7.040779 | 5.282323 |  6.056859 | 4.714119 | 2.349998 |  4.009121 | 8.512256 | ... |  3.577845 | 3.825272 | 11.283218 | 9.241972 |  2.816389 |  4.849044 | 10.391058 | 10.849582 |  2.787449 | 7.923482 |\n",
       "| GSM495213 | 3.087147 | 7.321854 | 6.483467 | 7.054886 | 5.469455 |  6.045071 | 4.713800 | 2.570193 |  4.100364 | 8.640070 | ... |  3.529783 | 3.634708 | 11.322901 | 9.206841 |  2.836462 |  4.920250 | 10.263996 | 10.778257 |  2.581674 | 8.127655 |\n",
       "| GSM495214 | 2.768613 | 7.277230 | 6.747241 | 7.079465 | 5.277774 |  5.821334 | 4.967451 | 2.819242 |  4.110803 | 8.554801 | ... |  3.700635 | 3.726439 | 11.288997 | 9.229028 |  2.599532 |  5.177272 | 10.423879 | 10.999074 |  2.230049 | 8.029409 |\n",
       "\n"
      ],
      "text/plain": [
       "          X1367566_at X1367568_a_at X1367570_at X1367584_at X1367614_at\n",
       "GSM495209 3.396192    7.685769      5.722330    6.628653    6.659618   \n",
       "GSM495210 2.882626    7.759002      6.005583    6.771917    6.655919   \n",
       "GSM495211 3.072980    7.598871      5.741630    6.564820    6.741483   \n",
       "GSM495212 3.168440    7.209959      6.396841    7.040779    5.282323   \n",
       "GSM495213 3.087147    7.321854      6.483467    7.054886    5.469455   \n",
       "GSM495214 2.768613    7.277230      6.747241    7.079465    5.277774   \n",
       "          X1367647_at X1367661_at X1367664_at X1367804_at X1367816_at ...\n",
       "GSM495209 12.737089   5.163315    3.101658    11.459725   4.819886    ...\n",
       "GSM495210 12.755694   5.491298    3.095527    11.439763   4.708422    ...\n",
       "GSM495211 12.725779   5.291097    2.864352    11.425470   4.753807    ...\n",
       "GSM495212  6.056859   4.714119    2.349998     4.009121   8.512256    ...\n",
       "GSM495213  6.045071   4.713800    2.570193     4.100364   8.640070    ...\n",
       "GSM495214  5.821334   4.967451    2.819242     4.110803   8.554801    ...\n",
       "          X1398282_at X1398318_at X1398357_at X1398368_at X1398514_at\n",
       "GSM495209 10.524515   4.401075     3.327705   4.043979    10.773481  \n",
       "GSM495210 10.548672   3.953334     3.605773   3.779552    10.819626  \n",
       "GSM495211 10.536997   4.766228     3.657362   4.435827    10.861409  \n",
       "GSM495212  3.577845   3.825272    11.283218   9.241972     2.816389  \n",
       "GSM495213  3.529783   3.634708    11.322901   9.206841     2.836462  \n",
       "GSM495214  3.700635   3.726439    11.288997   9.229028     2.599532  \n",
       "          X1398577_at X1398578_at X1398625_at X1398634_at X1398716_at\n",
       "GSM495209 12.232607    3.410767    2.587550   10.230386   2.275608   \n",
       "GSM495210 12.097947    3.605773    2.429901   10.233172   1.937317   \n",
       "GSM495211 12.218917    3.309184    2.611046   10.202474   1.919408   \n",
       "GSM495212  4.849044   10.391058   10.849582    2.787449   7.923482   \n",
       "GSM495213  4.920250   10.263996   10.778257    2.581674   8.127655   \n",
       "GSM495214  5.177272   10.423879   10.999074    2.230049   8.029409   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A matrix: 6 × 3 of type dbl</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Liver</th><th scope=col>Brain</th><th scope=col>Lung</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>GSM495209</th><td>1</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>GSM495210</th><td>1</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>GSM495211</th><td>1</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>GSM495212</th><td>0</td><td>1</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>GSM495213</th><td>0</td><td>1</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>GSM495214</th><td>0</td><td>1</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 6 × 3 of type dbl\n",
       "\\begin{tabular}{r|lll}\n",
       "  & Liver & Brain & Lung\\\\\n",
       "\\hline\n",
       "\tGSM495209 & 1 & 0 & 0\\\\\n",
       "\tGSM495210 & 1 & 0 & 0\\\\\n",
       "\tGSM495211 & 1 & 0 & 0\\\\\n",
       "\tGSM495212 & 0 & 1 & 0\\\\\n",
       "\tGSM495213 & 0 & 1 & 0\\\\\n",
       "\tGSM495214 & 0 & 1 & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 6 × 3 of type dbl\n",
       "\n",
       "| <!--/--> | Liver | Brain | Lung |\n",
       "|---|---|---|---|\n",
       "| GSM495209 | 1 | 0 | 0 |\n",
       "| GSM495210 | 1 | 0 | 0 |\n",
       "| GSM495211 | 1 | 0 | 0 |\n",
       "| GSM495212 | 0 | 1 | 0 |\n",
       "| GSM495213 | 0 | 1 | 0 |\n",
       "| GSM495214 | 0 | 1 | 0 |\n",
       "\n"
      ],
      "text/plain": [
       "          Liver Brain Lung\n",
       "GSM495209 1     0     0   \n",
       "GSM495210 1     0     0   \n",
       "GSM495211 1     0     0   \n",
       "GSM495212 0     1     0   \n",
       "GSM495213 0     1     0   \n",
       "GSM495214 0     1     0   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(shen_orr_ex$anno$mixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>GSM495209</dt><dd>1</dd><dt>GSM495210</dt><dd>1</dd><dt>GSM495211</dt><dd>1</dd><dt>GSM495212</dt><dd>0</dd><dt>GSM495213</dt><dd>0</dd><dt>GSM495214</dt><dd>0</dd><dt>GSM495215</dt><dd>0</dd><dt>GSM495216</dt><dd>0</dd><dt>GSM495217</dt><dd>0</dd><dt>GSM495218</dt><dd>0.05</dd><dt>GSM495219</dt><dd>0.05</dd><dt>GSM495220</dt><dd>0.05</dd><dt>GSM495221</dt><dd>0.7</dd><dt>GSM495222</dt><dd>0.7</dd><dt>GSM495223</dt><dd>0.7</dd><dt>GSM495224</dt><dd>0.25</dd><dt>GSM495225</dt><dd>0.25</dd><dt>GSM495226</dt><dd>0.25</dd><dt>GSM495227</dt><dd>0.7</dd><dt>GSM495228</dt><dd>0.7</dd><dt>GSM495229</dt><dd>0.7</dd><dt>GSM495230</dt><dd>0.45</dd><dt>GSM495231</dt><dd>0.45</dd><dt>GSM495232</dt><dd>0.45</dd><dt>GSM495233</dt><dd>0.55</dd><dt>GSM495234</dt><dd>0.55</dd><dt>GSM495235</dt><dd>0.55</dd><dt>GSM495236</dt><dd>0.5</dd><dt>GSM495237</dt><dd>0.5</dd><dt>GSM495238</dt><dd>0.5</dd><dt>GSM495239</dt><dd>0.55</dd><dt>GSM495240</dt><dd>0.55</dd><dt>GSM495241</dt><dd>0.55</dd><dt>GSM495242</dt><dd>0.5</dd><dt>GSM495243</dt><dd>0.5</dd><dt>GSM495244</dt><dd>0.5</dd><dt>GSM495245</dt><dd>0.6</dd><dt>GSM495246</dt><dd>0.6</dd><dt>GSM495247</dt><dd>0.6</dd><dt>GSM495248</dt><dd>0.65</dd><dt>GSM495249</dt><dd>0.65</dd><dt>GSM495250</dt><dd>0.65</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[GSM495209] 1\n",
       "\\item[GSM495210] 1\n",
       "\\item[GSM495211] 1\n",
       "\\item[GSM495212] 0\n",
       "\\item[GSM495213] 0\n",
       "\\item[GSM495214] 0\n",
       "\\item[GSM495215] 0\n",
       "\\item[GSM495216] 0\n",
       "\\item[GSM495217] 0\n",
       "\\item[GSM495218] 0.05\n",
       "\\item[GSM495219] 0.05\n",
       "\\item[GSM495220] 0.05\n",
       "\\item[GSM495221] 0.7\n",
       "\\item[GSM495222] 0.7\n",
       "\\item[GSM495223] 0.7\n",
       "\\item[GSM495224] 0.25\n",
       "\\item[GSM495225] 0.25\n",
       "\\item[GSM495226] 0.25\n",
       "\\item[GSM495227] 0.7\n",
       "\\item[GSM495228] 0.7\n",
       "\\item[GSM495229] 0.7\n",
       "\\item[GSM495230] 0.45\n",
       "\\item[GSM495231] 0.45\n",
       "\\item[GSM495232] 0.45\n",
       "\\item[GSM495233] 0.55\n",
       "\\item[GSM495234] 0.55\n",
       "\\item[GSM495235] 0.55\n",
       "\\item[GSM495236] 0.5\n",
       "\\item[GSM495237] 0.5\n",
       "\\item[GSM495238] 0.5\n",
       "\\item[GSM495239] 0.55\n",
       "\\item[GSM495240] 0.55\n",
       "\\item[GSM495241] 0.55\n",
       "\\item[GSM495242] 0.5\n",
       "\\item[GSM495243] 0.5\n",
       "\\item[GSM495244] 0.5\n",
       "\\item[GSM495245] 0.6\n",
       "\\item[GSM495246] 0.6\n",
       "\\item[GSM495247] 0.6\n",
       "\\item[GSM495248] 0.65\n",
       "\\item[GSM495249] 0.65\n",
       "\\item[GSM495250] 0.65\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "GSM495209\n",
       ":   1GSM495210\n",
       ":   1GSM495211\n",
       ":   1GSM495212\n",
       ":   0GSM495213\n",
       ":   0GSM495214\n",
       ":   0GSM495215\n",
       ":   0GSM495216\n",
       ":   0GSM495217\n",
       ":   0GSM495218\n",
       ":   0.05GSM495219\n",
       ":   0.05GSM495220\n",
       ":   0.05GSM495221\n",
       ":   0.7GSM495222\n",
       ":   0.7GSM495223\n",
       ":   0.7GSM495224\n",
       ":   0.25GSM495225\n",
       ":   0.25GSM495226\n",
       ":   0.25GSM495227\n",
       ":   0.7GSM495228\n",
       ":   0.7GSM495229\n",
       ":   0.7GSM495230\n",
       ":   0.45GSM495231\n",
       ":   0.45GSM495232\n",
       ":   0.45GSM495233\n",
       ":   0.55GSM495234\n",
       ":   0.55GSM495235\n",
       ":   0.55GSM495236\n",
       ":   0.5GSM495237\n",
       ":   0.5GSM495238\n",
       ":   0.5GSM495239\n",
       ":   0.55GSM495240\n",
       ":   0.55GSM495241\n",
       ":   0.55GSM495242\n",
       ":   0.5GSM495243\n",
       ":   0.5GSM495244\n",
       ":   0.5GSM495245\n",
       ":   0.6GSM495246\n",
       ":   0.6GSM495247\n",
       ":   0.6GSM495248\n",
       ":   0.65GSM495249\n",
       ":   0.65GSM495250\n",
       ":   0.65\n",
       "\n"
      ],
      "text/plain": [
       "GSM495209 GSM495210 GSM495211 GSM495212 GSM495213 GSM495214 GSM495215 GSM495216 \n",
       "     1.00      1.00      1.00      0.00      0.00      0.00      0.00      0.00 \n",
       "GSM495217 GSM495218 GSM495219 GSM495220 GSM495221 GSM495222 GSM495223 GSM495224 \n",
       "     0.00      0.05      0.05      0.05      0.70      0.70      0.70      0.25 \n",
       "GSM495225 GSM495226 GSM495227 GSM495228 GSM495229 GSM495230 GSM495231 GSM495232 \n",
       "     0.25      0.25      0.70      0.70      0.70      0.45      0.45      0.45 \n",
       "GSM495233 GSM495234 GSM495235 GSM495236 GSM495237 GSM495238 GSM495239 GSM495240 \n",
       "     0.55      0.55      0.55      0.50      0.50      0.50      0.55      0.55 \n",
       "GSM495241 GSM495242 GSM495243 GSM495244 GSM495245 GSM495246 GSM495247 GSM495248 \n",
       "     0.55      0.50      0.50      0.50      0.60      0.60      0.60      0.65 \n",
       "GSM495249 GSM495250 \n",
       "     0.65      0.65 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = shen_orr_ex$anno$mixture[,1]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.frame(cbind(y,X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 601</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>y</th><th scope=col>X1367566_at</th><th scope=col>X1367568_a_at</th><th scope=col>X1367570_at</th><th scope=col>X1367584_at</th><th scope=col>X1367614_at</th><th scope=col>X1367647_at</th><th scope=col>X1367661_at</th><th scope=col>X1367664_at</th><th scope=col>X1367804_at</th><th scope=col>...</th><th scope=col>X1398282_at</th><th scope=col>X1398318_at</th><th scope=col>X1398357_at</th><th scope=col>X1398368_at</th><th scope=col>X1398514_at</th><th scope=col>X1398577_at</th><th scope=col>X1398578_at</th><th scope=col>X1398625_at</th><th scope=col>X1398634_at</th><th scope=col>X1398716_at</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>...</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>GSM495245</th><td>0.60</td><td>11.565262</td><td>8.870303</td><td>7.001029</td><td>7.659077</td><td>7.948723</td><td>12.65228</td><td>8.051194</td><td>3.663120</td><td>10.93508</td><td>...</td><td>9.649767</td><td>5.979004</td><td>10.21558</td><td>8.246979</td><td>10.073258</td><td>11.94294</td><td>9.310983</td><td>10.023633</td><td>9.237172</td><td>7.217724</td></tr>\n",
       "\t<tr><th scope=row>GSM495246</th><td>0.60</td><td>11.539572</td><td>8.914555</td><td>7.090162</td><td>7.834538</td><td>7.908598</td><td>12.59200</td><td>8.198547</td><td>3.717454</td><td>10.92464</td><td>...</td><td>9.676873</td><td>6.070912</td><td>10.22762</td><td>8.196180</td><td>10.105497</td><td>11.88726</td><td>9.344754</td><td> 9.988609</td><td>9.347465</td><td>6.731695</td></tr>\n",
       "\t<tr><th scope=row>GSM495247</th><td>0.60</td><td>11.548155</td><td>8.887477</td><td>7.093302</td><td>7.813565</td><td>7.975642</td><td>12.59518</td><td>8.202095</td><td>3.635470</td><td>10.89393</td><td>...</td><td>9.639680</td><td>6.124330</td><td>10.22700</td><td>8.250561</td><td> 9.999937</td><td>11.98563</td><td>9.376115</td><td>10.123092</td><td>9.297599</td><td>7.074149</td></tr>\n",
       "\t<tr><th scope=row>GSM495248</th><td>0.65</td><td> 9.829747</td><td>7.720467</td><td>6.056899</td><td>6.736379</td><td>6.573709</td><td>12.67022</td><td>6.299485</td><td>3.227013</td><td>10.94865</td><td>...</td><td>9.765793</td><td>4.627772</td><td>10.23085</td><td>8.292535</td><td>10.207433</td><td>12.00831</td><td>9.328644</td><td> 9.958102</td><td>9.419154</td><td>7.047483</td></tr>\n",
       "\t<tr><th scope=row>GSM495249</th><td>0.65</td><td> 9.851467</td><td>7.664858</td><td>6.183294</td><td>6.735433</td><td>6.543160</td><td>12.68000</td><td>6.405826</td><td>3.100851</td><td>11.06752</td><td>...</td><td>9.822453</td><td>4.520327</td><td>10.16035</td><td>8.238181</td><td>10.119143</td><td>11.97799</td><td>9.305065</td><td> 9.953756</td><td>9.385342</td><td>6.968067</td></tr>\n",
       "\t<tr><th scope=row>GSM495250</th><td>0.65</td><td> 9.850309</td><td>7.699729</td><td>6.284626</td><td>6.842223</td><td>6.591180</td><td>12.63865</td><td>6.234437</td><td>2.462876</td><td>11.02859</td><td>...</td><td>9.838168</td><td>4.967616</td><td>10.10687</td><td>8.298151</td><td>10.108633</td><td>12.00717</td><td>9.331431</td><td> 9.947405</td><td>9.327041</td><td>7.217926</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 601\n",
       "\\begin{tabular}{r|lllllllllllllllllllll}\n",
       "  & y & X1367566\\_at & X1367568\\_a\\_at & X1367570\\_at & X1367584\\_at & X1367614\\_at & X1367647\\_at & X1367661\\_at & X1367664\\_at & X1367804\\_at & ... & X1398282\\_at & X1398318\\_at & X1398357\\_at & X1398368\\_at & X1398514\\_at & X1398577\\_at & X1398578\\_at & X1398625\\_at & X1398634\\_at & X1398716\\_at\\\\\n",
       "  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & ... & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\tGSM495245 & 0.60 & 11.565262 & 8.870303 & 7.001029 & 7.659077 & 7.948723 & 12.65228 & 8.051194 & 3.663120 & 10.93508 & ... & 9.649767 & 5.979004 & 10.21558 & 8.246979 & 10.073258 & 11.94294 & 9.310983 & 10.023633 & 9.237172 & 7.217724\\\\\n",
       "\tGSM495246 & 0.60 & 11.539572 & 8.914555 & 7.090162 & 7.834538 & 7.908598 & 12.59200 & 8.198547 & 3.717454 & 10.92464 & ... & 9.676873 & 6.070912 & 10.22762 & 8.196180 & 10.105497 & 11.88726 & 9.344754 &  9.988609 & 9.347465 & 6.731695\\\\\n",
       "\tGSM495247 & 0.60 & 11.548155 & 8.887477 & 7.093302 & 7.813565 & 7.975642 & 12.59518 & 8.202095 & 3.635470 & 10.89393 & ... & 9.639680 & 6.124330 & 10.22700 & 8.250561 &  9.999937 & 11.98563 & 9.376115 & 10.123092 & 9.297599 & 7.074149\\\\\n",
       "\tGSM495248 & 0.65 &  9.829747 & 7.720467 & 6.056899 & 6.736379 & 6.573709 & 12.67022 & 6.299485 & 3.227013 & 10.94865 & ... & 9.765793 & 4.627772 & 10.23085 & 8.292535 & 10.207433 & 12.00831 & 9.328644 &  9.958102 & 9.419154 & 7.047483\\\\\n",
       "\tGSM495249 & 0.65 &  9.851467 & 7.664858 & 6.183294 & 6.735433 & 6.543160 & 12.68000 & 6.405826 & 3.100851 & 11.06752 & ... & 9.822453 & 4.520327 & 10.16035 & 8.238181 & 10.119143 & 11.97799 & 9.305065 &  9.953756 & 9.385342 & 6.968067\\\\\n",
       "\tGSM495250 & 0.65 &  9.850309 & 7.699729 & 6.284626 & 6.842223 & 6.591180 & 12.63865 & 6.234437 & 2.462876 & 11.02859 & ... & 9.838168 & 4.967616 & 10.10687 & 8.298151 & 10.108633 & 12.00717 & 9.331431 &  9.947405 & 9.327041 & 7.217926\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 601\n",
       "\n",
       "| <!--/--> | y &lt;dbl&gt; | X1367566_at &lt;dbl&gt; | X1367568_a_at &lt;dbl&gt; | X1367570_at &lt;dbl&gt; | X1367584_at &lt;dbl&gt; | X1367614_at &lt;dbl&gt; | X1367647_at &lt;dbl&gt; | X1367661_at &lt;dbl&gt; | X1367664_at &lt;dbl&gt; | X1367804_at &lt;dbl&gt; | ... ... | X1398282_at &lt;dbl&gt; | X1398318_at &lt;dbl&gt; | X1398357_at &lt;dbl&gt; | X1398368_at &lt;dbl&gt; | X1398514_at &lt;dbl&gt; | X1398577_at &lt;dbl&gt; | X1398578_at &lt;dbl&gt; | X1398625_at &lt;dbl&gt; | X1398634_at &lt;dbl&gt; | X1398716_at &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| GSM495245 | 0.60 | 11.565262 | 8.870303 | 7.001029 | 7.659077 | 7.948723 | 12.65228 | 8.051194 | 3.663120 | 10.93508 | ... | 9.649767 | 5.979004 | 10.21558 | 8.246979 | 10.073258 | 11.94294 | 9.310983 | 10.023633 | 9.237172 | 7.217724 |\n",
       "| GSM495246 | 0.60 | 11.539572 | 8.914555 | 7.090162 | 7.834538 | 7.908598 | 12.59200 | 8.198547 | 3.717454 | 10.92464 | ... | 9.676873 | 6.070912 | 10.22762 | 8.196180 | 10.105497 | 11.88726 | 9.344754 |  9.988609 | 9.347465 | 6.731695 |\n",
       "| GSM495247 | 0.60 | 11.548155 | 8.887477 | 7.093302 | 7.813565 | 7.975642 | 12.59518 | 8.202095 | 3.635470 | 10.89393 | ... | 9.639680 | 6.124330 | 10.22700 | 8.250561 |  9.999937 | 11.98563 | 9.376115 | 10.123092 | 9.297599 | 7.074149 |\n",
       "| GSM495248 | 0.65 |  9.829747 | 7.720467 | 6.056899 | 6.736379 | 6.573709 | 12.67022 | 6.299485 | 3.227013 | 10.94865 | ... | 9.765793 | 4.627772 | 10.23085 | 8.292535 | 10.207433 | 12.00831 | 9.328644 |  9.958102 | 9.419154 | 7.047483 |\n",
       "| GSM495249 | 0.65 |  9.851467 | 7.664858 | 6.183294 | 6.735433 | 6.543160 | 12.68000 | 6.405826 | 3.100851 | 11.06752 | ... | 9.822453 | 4.520327 | 10.16035 | 8.238181 | 10.119143 | 11.97799 | 9.305065 |  9.953756 | 9.385342 | 6.968067 |\n",
       "| GSM495250 | 0.65 |  9.850309 | 7.699729 | 6.284626 | 6.842223 | 6.591180 | 12.63865 | 6.234437 | 2.462876 | 11.02859 | ... | 9.838168 | 4.967616 | 10.10687 | 8.298151 | 10.108633 | 12.00717 | 9.331431 |  9.947405 | 9.327041 | 7.217926 |\n",
       "\n"
      ],
      "text/plain": [
       "          y    X1367566_at X1367568_a_at X1367570_at X1367584_at X1367614_at\n",
       "GSM495245 0.60 11.565262   8.870303      7.001029    7.659077    7.948723   \n",
       "GSM495246 0.60 11.539572   8.914555      7.090162    7.834538    7.908598   \n",
       "GSM495247 0.60 11.548155   8.887477      7.093302    7.813565    7.975642   \n",
       "GSM495248 0.65  9.829747   7.720467      6.056899    6.736379    6.573709   \n",
       "GSM495249 0.65  9.851467   7.664858      6.183294    6.735433    6.543160   \n",
       "GSM495250 0.65  9.850309   7.699729      6.284626    6.842223    6.591180   \n",
       "          X1367647_at X1367661_at X1367664_at X1367804_at ... X1398282_at\n",
       "GSM495245 12.65228    8.051194    3.663120    10.93508    ... 9.649767   \n",
       "GSM495246 12.59200    8.198547    3.717454    10.92464    ... 9.676873   \n",
       "GSM495247 12.59518    8.202095    3.635470    10.89393    ... 9.639680   \n",
       "GSM495248 12.67022    6.299485    3.227013    10.94865    ... 9.765793   \n",
       "GSM495249 12.68000    6.405826    3.100851    11.06752    ... 9.822453   \n",
       "GSM495250 12.63865    6.234437    2.462876    11.02859    ... 9.838168   \n",
       "          X1398318_at X1398357_at X1398368_at X1398514_at X1398577_at\n",
       "GSM495245 5.979004    10.21558    8.246979    10.073258   11.94294   \n",
       "GSM495246 6.070912    10.22762    8.196180    10.105497   11.88726   \n",
       "GSM495247 6.124330    10.22700    8.250561     9.999937   11.98563   \n",
       "GSM495248 4.627772    10.23085    8.292535    10.207433   12.00831   \n",
       "GSM495249 4.520327    10.16035    8.238181    10.119143   11.97799   \n",
       "GSM495250 4.967616    10.10687    8.298151    10.108633   12.00717   \n",
       "          X1398578_at X1398625_at X1398634_at X1398716_at\n",
       "GSM495245 9.310983    10.023633   9.237172    7.217724   \n",
       "GSM495246 9.344754     9.988609   9.347465    6.731695   \n",
       "GSM495247 9.376115    10.123092   9.297599    7.074149   \n",
       "GSM495248 9.328644     9.958102   9.419154    7.047483   \n",
       "GSM495249 9.305065     9.953756   9.385342    6.968067   \n",
       "GSM495250 9.331431     9.947405   9.327041    7.217926   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tail(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>42</li><li>601</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 42\n",
       "\\item 601\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 42\n",
       "2. 601\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]  42 601"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = y ~ ., data = df)\n",
       "\n",
       "Residuals:\n",
       "ALL 42 residuals are 0: no residual degrees of freedom!\n",
       "\n",
       "Coefficients: (559 not defined because of singularities)\n",
       "               Estimate Std. Error t value Pr(>|t|)\n",
       "(Intercept)    3.673220         NA      NA       NA\n",
       "X1367566_at    0.067514         NA      NA       NA\n",
       "X1367568_a_at -0.288128         NA      NA       NA\n",
       "X1367570_at    0.074801         NA      NA       NA\n",
       "X1367584_at    0.011712         NA      NA       NA\n",
       "X1367614_at   -0.035175         NA      NA       NA\n",
       "X1367647_at    0.356097         NA      NA       NA\n",
       "X1367661_at    0.140001         NA      NA       NA\n",
       "X1367664_at    0.036364         NA      NA       NA\n",
       "X1367804_at    0.173166         NA      NA       NA\n",
       "X1367816_at   -0.209697         NA      NA       NA\n",
       "X1367838_at   -0.153792         NA      NA       NA\n",
       "X1367845_at   -0.182829         NA      NA       NA\n",
       "X1367846_at    0.037340         NA      NA       NA\n",
       "X1367851_at    0.492110         NA      NA       NA\n",
       "X1367871_at   -0.186297         NA      NA       NA\n",
       "X1367917_at    0.207761         NA      NA       NA\n",
       "X1367918_at    0.273023         NA      NA       NA\n",
       "X1367930_at    0.787954         NA      NA       NA\n",
       "X1367977_at   -0.436989         NA      NA       NA\n",
       "X1367988_at   -0.346595         NA      NA       NA\n",
       "X1367992_at   -0.518111         NA      NA       NA\n",
       "X1368034_at   -0.212905         NA      NA       NA\n",
       "X1368044_at    0.139855         NA      NA       NA\n",
       "X1368048_at    0.623739         NA      NA       NA\n",
       "X1368052_at   -0.003073         NA      NA       NA\n",
       "X1368077_at   -0.021572         NA      NA       NA\n",
       "X1368093_at   -0.132140         NA      NA       NA\n",
       "X1368097_a_at -0.243828         NA      NA       NA\n",
       "X1368114_at   -0.035573         NA      NA       NA\n",
       "X1368145_at   -0.382764         NA      NA       NA\n",
       "X1368155_at   -0.427857         NA      NA       NA\n",
       "X1368157_at   -0.144236         NA      NA       NA\n",
       "X1368160_at    0.026377         NA      NA       NA\n",
       "X1368161_a_at -0.506675         NA      NA       NA\n",
       "X1368168_at    0.066331         NA      NA       NA\n",
       "X1368170_at    0.398404         NA      NA       NA\n",
       "X1368171_at   -0.054135         NA      NA       NA\n",
       "X1368172_a_at -0.054183         NA      NA       NA\n",
       "X1368180_s_at  0.139921         NA      NA       NA\n",
       "X1368187_at    0.306661         NA      NA       NA\n",
       "X1368188_at   -0.026376         NA      NA       NA\n",
       "X1368205_at          NA         NA      NA       NA\n",
       "X1368224_at          NA         NA      NA       NA\n",
       "X1368245_at          NA         NA      NA       NA\n",
       "X1368256_at          NA         NA      NA       NA\n",
       "X1368263_a_at        NA         NA      NA       NA\n",
       "X1368281_at          NA         NA      NA       NA\n",
       "X1368282_at          NA         NA      NA       NA\n",
       "X1368283_at          NA         NA      NA       NA\n",
       "X1368284_at          NA         NA      NA       NA\n",
       "X1368288_at          NA         NA      NA       NA\n",
       "X1368289_at          NA         NA      NA       NA\n",
       "X1368316_at          NA         NA      NA       NA\n",
       "X1368335_at          NA         NA      NA       NA\n",
       "X1368339_at          NA         NA      NA       NA\n",
       "X1368344_at          NA         NA      NA       NA\n",
       "X1368353_at          NA         NA      NA       NA\n",
       "X1368360_at          NA         NA      NA       NA\n",
       "X1368362_a_at        NA         NA      NA       NA\n",
       "X1368395_at          NA         NA      NA       NA\n",
       "X1368397_at          NA         NA      NA       NA\n",
       "X1368401_at          NA         NA      NA       NA\n",
       "X1368413_at          NA         NA      NA       NA\n",
       "X1368423_at          NA         NA      NA       NA\n",
       "X1368435_at          NA         NA      NA       NA\n",
       "X1368442_at          NA         NA      NA       NA\n",
       "X1368458_at          NA         NA      NA       NA\n",
       "X1368467_at          NA         NA      NA       NA\n",
       "X1368469_at          NA         NA      NA       NA\n",
       "X1368473_at          NA         NA      NA       NA\n",
       "X1368497_at          NA         NA      NA       NA\n",
       "X1368520_at          NA         NA      NA       NA\n",
       "X1368521_at          NA         NA      NA       NA\n",
       "X1368524_at          NA         NA      NA       NA\n",
       "X1368553_at          NA         NA      NA       NA\n",
       "X1368564_at          NA         NA      NA       NA\n",
       "X1368569_at          NA         NA      NA       NA\n",
       "X1368577_at          NA         NA      NA       NA\n",
       "X1368583_a_at        NA         NA      NA       NA\n",
       "X1368587_at          NA         NA      NA       NA\n",
       "X1368609_at          NA         NA      NA       NA\n",
       "X1368627_at          NA         NA      NA       NA\n",
       "X1368632_at          NA         NA      NA       NA\n",
       "X1368656_at          NA         NA      NA       NA\n",
       "X1368659_at          NA         NA      NA       NA\n",
       "X1368686_at          NA         NA      NA       NA\n",
       "X1368695_at          NA         NA      NA       NA\n",
       "X1368706_at          NA         NA      NA       NA\n",
       "X1368707_at          NA         NA      NA       NA\n",
       "X1368720_at          NA         NA      NA       NA\n",
       "X1368731_at          NA         NA      NA       NA\n",
       "X1368741_at          NA         NA      NA       NA\n",
       "X1368769_at          NA         NA      NA       NA\n",
       "X1368790_at          NA         NA      NA       NA\n",
       "X1368810_a_at        NA         NA      NA       NA\n",
       "X1368829_at          NA         NA      NA       NA\n",
       "X1368853_at          NA         NA      NA       NA\n",
       "X1368858_at          NA         NA      NA       NA\n",
       "X1368864_at          NA         NA      NA       NA\n",
       "X1368865_at          NA         NA      NA       NA\n",
       "X1368887_at          NA         NA      NA       NA\n",
       "X1368901_at          NA         NA      NA       NA\n",
       "X1368934_at          NA         NA      NA       NA\n",
       "X1368948_at          NA         NA      NA       NA\n",
       "X1369011_at          NA         NA      NA       NA\n",
       "X1369029_at          NA         NA      NA       NA\n",
       "X1369074_at          NA         NA      NA       NA\n",
       "X1369085_s_at        NA         NA      NA       NA\n",
       "X1369107_at          NA         NA      NA       NA\n",
       "X1369111_at          NA         NA      NA       NA\n",
       "X1369203_at          NA         NA      NA       NA\n",
       "X1369206_at          NA         NA      NA       NA\n",
       "X1369210_at          NA         NA      NA       NA\n",
       "X1369225_at          NA         NA      NA       NA\n",
       "X1369275_s_at        NA         NA      NA       NA\n",
       "X1369286_at          NA         NA      NA       NA\n",
       "X1369435_at          NA         NA      NA       NA\n",
       "X1369492_at          NA         NA      NA       NA\n",
       "X1369493_at          NA         NA      NA       NA\n",
       "X1369502_a_at        NA         NA      NA       NA\n",
       "X1369509_a_at        NA         NA      NA       NA\n",
       "X1369520_a_at        NA         NA      NA       NA\n",
       "X1369546_at          NA         NA      NA       NA\n",
       "X1369581_at          NA         NA      NA       NA\n",
       "X1369627_at          NA         NA      NA       NA\n",
       "X1369647_at          NA         NA      NA       NA\n",
       "X1369648_at          NA         NA      NA       NA\n",
       "X1369651_at          NA         NA      NA       NA\n",
       "X1369652_at          NA         NA      NA       NA\n",
       "X1369657_at          NA         NA      NA       NA\n",
       "X1369662_at          NA         NA      NA       NA\n",
       "X1369671_at          NA         NA      NA       NA\n",
       "X1369701_at          NA         NA      NA       NA\n",
       "X1369727_at          NA         NA      NA       NA\n",
       "X1369746_a_at        NA         NA      NA       NA\n",
       "X1369765_at          NA         NA      NA       NA\n",
       "X1369790_at          NA         NA      NA       NA\n",
       "X1369837_at          NA         NA      NA       NA\n",
       "X1369840_at          NA         NA      NA       NA\n",
       "X1369852_at          NA         NA      NA       NA\n",
       "X1369861_at          NA         NA      NA       NA\n",
       "X1369926_at          NA         NA      NA       NA\n",
       "X1369977_at          NA         NA      NA       NA\n",
       "X1370009_at          NA         NA      NA       NA\n",
       "X1370016_at          NA         NA      NA       NA\n",
       "X1370027_a_at        NA         NA      NA       NA\n",
       "X1370028_at          NA         NA      NA       NA\n",
       "X1370041_at          NA         NA      NA       NA\n",
       "X1370042_at          NA         NA      NA       NA\n",
       "X1370056_at          NA         NA      NA       NA\n",
       "X1370058_at          NA         NA      NA       NA\n",
       "X1370059_at          NA         NA      NA       NA\n",
       "X1370065_at          NA         NA      NA       NA\n",
       "X1370069_at          NA         NA      NA       NA\n",
       "X1370072_at          NA         NA      NA       NA\n",
       "X1370097_a_at        NA         NA      NA       NA\n",
       "X1370124_at          NA         NA      NA       NA\n",
       "X1370131_at          NA         NA      NA       NA\n",
       "X1370135_at          NA         NA      NA       NA\n",
       "X1370146_at          NA         NA      NA       NA\n",
       "X1370149_at          NA         NA      NA       NA\n",
       "X1370151_at          NA         NA      NA       NA\n",
       "X1370154_at          NA         NA      NA       NA\n",
       "X1370155_at          NA         NA      NA       NA\n",
       "X1370201_at          NA         NA      NA       NA\n",
       "X1370229_at          NA         NA      NA       NA\n",
       "X1370241_at          NA         NA      NA       NA\n",
       "X1370255_at          NA         NA      NA       NA\n",
       "X1370257_at          NA         NA      NA       NA\n",
       "X1370299_at          NA         NA      NA       NA\n",
       "X1370301_at          NA         NA      NA       NA\n",
       "X1370341_at          NA         NA      NA       NA\n",
       "X1370350_x_at        NA         NA      NA       NA\n",
       "X1370359_at          NA         NA      NA       NA\n",
       "X1370377_at          NA         NA      NA       NA\n",
       "X1370394_at          NA         NA      NA       NA\n",
       "X1370397_at          NA         NA      NA       NA\n",
       "X1370399_at          NA         NA      NA       NA\n",
       "X1370401_at          NA         NA      NA       NA\n",
       "X1370434_a_at        NA         NA      NA       NA\n",
       "X1370439_a_at        NA         NA      NA       NA\n",
       "X1370455_a_at        NA         NA      NA       NA\n",
       "X1370496_at          NA         NA      NA       NA\n",
       "X1370500_a_at        NA         NA      NA       NA\n",
       "X1370511_at          NA         NA      NA       NA\n",
       "X1370517_at          NA         NA      NA       NA\n",
       "X1370547_at          NA         NA      NA       NA\n",
       "X1370555_at          NA         NA      NA       NA\n",
       "X1370580_a_at        NA         NA      NA       NA\n",
       "X1370592_at          NA         NA      NA       NA\n",
       "X1370676_at          NA         NA      NA       NA\n",
       "X1370698_at          NA         NA      NA       NA\n",
       "X1370725_a_at        NA         NA      NA       NA\n",
       "X1370777_at          NA         NA      NA       NA\n",
       "X1370789_a_at        NA         NA      NA       NA\n",
       "X1370815_at          NA         NA      NA       NA\n",
       "X1370836_at          NA         NA      NA       NA\n",
       "X1370856_at          NA         NA      NA       NA\n",
       "X1370869_at          NA         NA      NA       NA\n",
       "X1370895_at          NA         NA      NA       NA\n",
       "X1370936_at          NA         NA      NA       NA\n",
       "X1370959_at          NA         NA      NA       NA\n",
       "X1370967_at          NA         NA      NA       NA\n",
       "X1370969_at          NA         NA      NA       NA\n",
       "X1370973_at          NA         NA      NA       NA\n",
       "X1370980_at          NA         NA      NA       NA\n",
       "X1370992_a_at        NA         NA      NA       NA\n",
       "X1371010_at          NA         NA      NA       NA\n",
       "X1371025_at          NA         NA      NA       NA\n",
       "X1371030_at          NA         NA      NA       NA\n",
       "X1371031_at          NA         NA      NA       NA\n",
       "X1371034_at          NA         NA      NA       NA\n",
       "X1371050_at          NA         NA      NA       NA\n",
       "X1371076_at          NA         NA      NA       NA\n",
       "X1371083_at          NA         NA      NA       NA\n",
       "X1371098_a_at        NA         NA      NA       NA\n",
       "X1371100_at          NA         NA      NA       NA\n",
       "X1371109_at          NA         NA      NA       NA\n",
       "X1371143_at          NA         NA      NA       NA\n",
       "X1371147_at          NA         NA      NA       NA\n",
       "X1371258_at          NA         NA      NA       NA\n",
       "X1371266_at          NA         NA      NA       NA\n",
       "X1371315_at          NA         NA      NA       NA\n",
       "X1371414_at          NA         NA      NA       NA\n",
       "X1371527_at          NA         NA      NA       NA\n",
       "X1371575_at          NA         NA      NA       NA\n",
       "X1371700_at          NA         NA      NA       NA\n",
       "X1371703_at          NA         NA      NA       NA\n",
       "X1371923_at          NA         NA      NA       NA\n",
       "X1371970_at          NA         NA      NA       NA\n",
       "X1372111_at          NA         NA      NA       NA\n",
       "X1372118_at          NA         NA      NA       NA\n",
       "X1372163_at          NA         NA      NA       NA\n",
       "X1372256_at          NA         NA      NA       NA\n",
       "X1372264_at          NA         NA      NA       NA\n",
       "X1372294_at          NA         NA      NA       NA\n",
       "X1372345_at          NA         NA      NA       NA\n",
       "X1372439_at          NA         NA      NA       NA\n",
       "X1372615_at          NA         NA      NA       NA\n",
       "X1372725_at          NA         NA      NA       NA\n",
       "X1372936_at          NA         NA      NA       NA\n",
       "X1373098_at          NA         NA      NA       NA\n",
       "X1373187_at          NA         NA      NA       NA\n",
       "X1373202_at          NA         NA      NA       NA\n",
       "X1373266_at          NA         NA      NA       NA\n",
       "X1373326_at          NA         NA      NA       NA\n",
       "X1373333_at          NA         NA      NA       NA\n",
       "X1373463_at          NA         NA      NA       NA\n",
       "X1373617_at          NA         NA      NA       NA\n",
       "X1373654_at          NA         NA      NA       NA\n",
       "X1373661_a_at        NA         NA      NA       NA\n",
       "X1373674_at          NA         NA      NA       NA\n",
       "X1373686_at          NA         NA      NA       NA\n",
       "X1373710_at          NA         NA      NA       NA\n",
       "X1373740_at          NA         NA      NA       NA\n",
       "X1373774_at          NA         NA      NA       NA\n",
       "X1373783_at          NA         NA      NA       NA\n",
       "X1373865_at          NA         NA      NA       NA\n",
       "X1373896_at          NA         NA      NA       NA\n",
       "X1373900_at          NA         NA      NA       NA\n",
       "X1373908_at          NA         NA      NA       NA\n",
       "X1373911_at          NA         NA      NA       NA\n",
       "X1373945_at          NA         NA      NA       NA\n",
       "X1373977_at          NA         NA      NA       NA\n",
       "X1374070_at          NA         NA      NA       NA\n",
       "X1374122_at          NA         NA      NA       NA\n",
       "X1374187_at          NA         NA      NA       NA\n",
       "X1374594_at          NA         NA      NA       NA\n",
       "X1374630_at          NA         NA      NA       NA\n",
       "X1374816_at          NA         NA      NA       NA\n",
       "X1375170_at          NA         NA      NA       NA\n",
       "X1375267_at          NA         NA      NA       NA\n",
       "X1375367_at          NA         NA      NA       NA\n",
       "X1375575_at          NA         NA      NA       NA\n",
       "X1375707_at          NA         NA      NA       NA\n",
       "X1375861_at          NA         NA      NA       NA\n",
       "X1375877_at          NA         NA      NA       NA\n",
       "X1375905_at          NA         NA      NA       NA\n",
       "X1375951_at          NA         NA      NA       NA\n",
       "X1376082_at          NA         NA      NA       NA\n",
       "X1376174_at          NA         NA      NA       NA\n",
       "X1376191_at          NA         NA      NA       NA\n",
       "X1376232_at          NA         NA      NA       NA\n",
       "X1376239_at          NA         NA      NA       NA\n",
       "X1376283_at          NA         NA      NA       NA\n",
       "X1376344_at          NA         NA      NA       NA\n",
       "X1376345_at          NA         NA      NA       NA\n",
       "X1376434_at          NA         NA      NA       NA\n",
       "X1376577_at          NA         NA      NA       NA\n",
       "X1376828_at          NA         NA      NA       NA\n",
       "X1376873_at          NA         NA      NA       NA\n",
       "X1376893_at          NA         NA      NA       NA\n",
       "X1376980_at          NA         NA      NA       NA\n",
       "X1377008_at          NA         NA      NA       NA\n",
       "X1377033_at          NA         NA      NA       NA\n",
       "X1377125_at          NA         NA      NA       NA\n",
       "X1377139_at          NA         NA      NA       NA\n",
       "X1377286_at          NA         NA      NA       NA\n",
       "X1377333_at          NA         NA      NA       NA\n",
       "X1377445_at          NA         NA      NA       NA\n",
       "X1377615_at          NA         NA      NA       NA\n",
       "X1377635_at          NA         NA      NA       NA\n",
       "X1377642_at          NA         NA      NA       NA\n",
       "X1377722_at          NA         NA      NA       NA\n",
       "X1377752_at          NA         NA      NA       NA\n",
       "X1377773_at          NA         NA      NA       NA\n",
       "X1377828_at          NA         NA      NA       NA\n",
       "X1377874_at          NA         NA      NA       NA\n",
       "X1377975_at          NA         NA      NA       NA\n",
       "X1378065_at          NA         NA      NA       NA\n",
       "X1378260_at          NA         NA      NA       NA\n",
       "X1378292_at          NA         NA      NA       NA\n",
       "X1378341_at          NA         NA      NA       NA\n",
       "X1378484_at          NA         NA      NA       NA\n",
       "X1378546_at          NA         NA      NA       NA\n",
       "X1379240_at          NA         NA      NA       NA\n",
       "X1379326_at          NA         NA      NA       NA\n",
       "X1379340_at          NA         NA      NA       NA\n",
       "X1379365_at          NA         NA      NA       NA\n",
       "X1379374_at          NA         NA      NA       NA\n",
       "X1379464_at          NA         NA      NA       NA\n",
       "X1379732_at          NA         NA      NA       NA\n",
       "X1379744_at          NA         NA      NA       NA\n",
       "X1379772_at          NA         NA      NA       NA\n",
       "X1379863_at          NA         NA      NA       NA\n",
       "X1379888_at          NA         NA      NA       NA\n",
       "X1379903_at          NA         NA      NA       NA\n",
       "X1379906_at          NA         NA      NA       NA\n",
       "X1379907_at          NA         NA      NA       NA\n",
       "X1380066_at          NA         NA      NA       NA\n",
       "X1380085_at          NA         NA      NA       NA\n",
       "X1380104_at          NA         NA      NA       NA\n",
       "X1380172_at          NA         NA      NA       NA\n",
       "X1380278_at          NA         NA      NA       NA\n",
       "X1380366_at          NA         NA      NA       NA\n",
       "X1380455_at          NA         NA      NA       NA\n",
       "X1380828_at          NA         NA      NA       NA\n",
       "X1380866_at          NA         NA      NA       NA\n",
       "X1381318_at          NA         NA      NA       NA\n",
       "X1381556_at          NA         NA      NA       NA\n",
       "X1381615_at          NA         NA      NA       NA\n",
       "X1381852_at          NA         NA      NA       NA\n",
       "X1381955_at          NA         NA      NA       NA\n",
       "X1381995_at          NA         NA      NA       NA\n",
       "X1382031_at          NA         NA      NA       NA\n",
       "X1382050_at          NA         NA      NA       NA\n",
       "X1382072_at          NA         NA      NA       NA\n",
       "X1382132_at          NA         NA      NA       NA\n",
       "X1382205_at          NA         NA      NA       NA\n",
       "X1382211_at          NA         NA      NA       NA\n",
       "X1382218_at          NA         NA      NA       NA\n",
       "X1382329_at          NA         NA      NA       NA\n",
       "X1382382_at          NA         NA      NA       NA\n",
       "X1382439_at          NA         NA      NA       NA\n",
       "X1382452_at          NA         NA      NA       NA\n",
       "X1382467_at          NA         NA      NA       NA\n",
       "X1382517_at          NA         NA      NA       NA\n",
       "X1382554_at          NA         NA      NA       NA\n",
       "X1382651_at          NA         NA      NA       NA\n",
       "X1382678_at          NA         NA      NA       NA\n",
       "X1382692_at          NA         NA      NA       NA\n",
       "X1382833_at          NA         NA      NA       NA\n",
       "X1382882_x_at        NA         NA      NA       NA\n",
       "X1383003_at          NA         NA      NA       NA\n",
       "X1383046_at          NA         NA      NA       NA\n",
       "X1383066_at          NA         NA      NA       NA\n",
       "X1383130_at          NA         NA      NA       NA\n",
       "X1383145_at          NA         NA      NA       NA\n",
       "X1383185_at          NA         NA      NA       NA\n",
       "X1383195_at          NA         NA      NA       NA\n",
       "X1383218_at          NA         NA      NA       NA\n",
       "X1383220_at          NA         NA      NA       NA\n",
       "X1383249_at          NA         NA      NA       NA\n",
       "X1383254_at          NA         NA      NA       NA\n",
       "X1383257_at          NA         NA      NA       NA\n",
       "X1383291_at          NA         NA      NA       NA\n",
       "X1383395_at          NA         NA      NA       NA\n",
       "X1383444_at          NA         NA      NA       NA\n",
       "X1383468_at          NA         NA      NA       NA\n",
       "X1383488_at          NA         NA      NA       NA\n",
       "X1383511_at          NA         NA      NA       NA\n",
       "X1383562_at          NA         NA      NA       NA\n",
       "X1383641_at          NA         NA      NA       NA\n",
       "X1383673_at          NA         NA      NA       NA\n",
       "X1383732_at          NA         NA      NA       NA\n",
       "X1383736_at          NA         NA      NA       NA\n",
       "X1383768_at          NA         NA      NA       NA\n",
       "X1383783_at          NA         NA      NA       NA\n",
       "X1383794_at          NA         NA      NA       NA\n",
       "X1383796_at          NA         NA      NA       NA\n",
       "X1383879_at          NA         NA      NA       NA\n",
       "X1383904_at          NA         NA      NA       NA\n",
       "X1384025_at          NA         NA      NA       NA\n",
       "X1384233_at          NA         NA      NA       NA\n",
       "X1384247_at          NA         NA      NA       NA\n",
       "X1384302_at          NA         NA      NA       NA\n",
       "X1384325_at          NA         NA      NA       NA\n",
       "X1384334_at          NA         NA      NA       NA\n",
       "X1384346_at          NA         NA      NA       NA\n",
       "X1384415_at          NA         NA      NA       NA\n",
       "X1384417_at          NA         NA      NA       NA\n",
       "X1384499_at          NA         NA      NA       NA\n",
       "X1384510_at          NA         NA      NA       NA\n",
       "X1384558_at          NA         NA      NA       NA\n",
       "X1384664_at          NA         NA      NA       NA\n",
       "X1385031_at          NA         NA      NA       NA\n",
       "X1385107_at          NA         NA      NA       NA\n",
       "X1385235_at          NA         NA      NA       NA\n",
       "X1385247_at          NA         NA      NA       NA\n",
       "X1385547_at          NA         NA      NA       NA\n",
       "X1385559_at          NA         NA      NA       NA\n",
       "X1385560_at          NA         NA      NA       NA\n",
       "X1385584_at          NA         NA      NA       NA\n",
       "X1385635_at          NA         NA      NA       NA\n",
       "X1385707_at          NA         NA      NA       NA\n",
       "X1385756_at          NA         NA      NA       NA\n",
       "X1385759_at          NA         NA      NA       NA\n",
       "X1386869_at          NA         NA      NA       NA\n",
       "X1386903_at          NA         NA      NA       NA\n",
       "X1386913_at          NA         NA      NA       NA\n",
       "X1386944_a_at        NA         NA      NA       NA\n",
       "X1386955_at          NA         NA      NA       NA\n",
       "X1386980_at          NA         NA      NA       NA\n",
       "X1386998_at          NA         NA      NA       NA\n",
       "X1387006_at          NA         NA      NA       NA\n",
       "X1387025_at          NA         NA      NA       NA\n",
       "X1387032_at          NA         NA      NA       NA\n",
       "X1387034_at          NA         NA      NA       NA\n",
       "X1387073_at          NA         NA      NA       NA\n",
       "X1387112_at          NA         NA      NA       NA\n",
       "X1387118_at          NA         NA      NA       NA\n",
       "X1387125_at          NA         NA      NA       NA\n",
       "X1387159_at          NA         NA      NA       NA\n",
       "X1387171_at          NA         NA      NA       NA\n",
       "X1387200_at          NA         NA      NA       NA\n",
       "X1387202_at          NA         NA      NA       NA\n",
       "X1387223_at          NA         NA      NA       NA\n",
       "X1387226_at          NA         NA      NA       NA\n",
       "X1387228_at          NA         NA      NA       NA\n",
       "X1387234_at          NA         NA      NA       NA\n",
       "X1387240_at          NA         NA      NA       NA\n",
       "X1387243_at          NA         NA      NA       NA\n",
       "X1387284_at          NA         NA      NA       NA\n",
       "X1387288_at          NA         NA      NA       NA\n",
       "X1387307_at          NA         NA      NA       NA\n",
       "X1387314_at          NA         NA      NA       NA\n",
       "X1387315_at          NA         NA      NA       NA\n",
       "X1387319_at          NA         NA      NA       NA\n",
       "X1387323_at          NA         NA      NA       NA\n",
       "X1387325_at          NA         NA      NA       NA\n",
       "X1387341_a_at        NA         NA      NA       NA\n",
       "X1387383_at          NA         NA      NA       NA\n",
       "X1387396_at          NA         NA      NA       NA\n",
       "X1387460_at          NA         NA      NA       NA\n",
       "X1387508_at          NA         NA      NA       NA\n",
       "X1387529_a_at        NA         NA      NA       NA\n",
       "X1387567_at          NA         NA      NA       NA\n",
       "X1387631_at          NA         NA      NA       NA\n",
       "X1387665_at          NA         NA      NA       NA\n",
       "X1387672_at          NA         NA      NA       NA\n",
       "X1387679_at          NA         NA      NA       NA\n",
       "X1387687_at          NA         NA      NA       NA\n",
       "X1387725_at          NA         NA      NA       NA\n",
       "X1387765_at          NA         NA      NA       NA\n",
       "X1387791_at          NA         NA      NA       NA\n",
       "X1387816_at          NA         NA      NA       NA\n",
       "X1387825_at          NA         NA      NA       NA\n",
       "X1387830_at          NA         NA      NA       NA\n",
       "X1387854_at          NA         NA      NA       NA\n",
       "X1387877_at          NA         NA      NA       NA\n",
       "X1387902_a_at        NA         NA      NA       NA\n",
       "X1387927_a_at        NA         NA      NA       NA\n",
       "X1387949_at          NA         NA      NA       NA\n",
       "X1387951_at          NA         NA      NA       NA\n",
       "X1387955_at          NA         NA      NA       NA\n",
       "X1387960_at          NA         NA      NA       NA\n",
       "X1387963_a_at        NA         NA      NA       NA\n",
       "X1387968_at          NA         NA      NA       NA\n",
       "X1387994_at          NA         NA      NA       NA\n",
       "X1388000_at          NA         NA      NA       NA\n",
       "X1388033_at          NA         NA      NA       NA\n",
       "X1388037_at          NA         NA      NA       NA\n",
       "X1388116_at          NA         NA      NA       NA\n",
       "X1388146_at          NA         NA      NA       NA\n",
       "X1388152_at          NA         NA      NA       NA\n",
       "X1388190_at          NA         NA      NA       NA\n",
       "X1388224_at          NA         NA      NA       NA\n",
       "X1388229_a_at        NA         NA      NA       NA\n",
       "X1388272_at          NA         NA      NA       NA\n",
       "X1388433_at          NA         NA      NA       NA\n",
       "X1388557_at          NA         NA      NA       NA\n",
       "X1388670_at          NA         NA      NA       NA\n",
       "X1388811_at          NA         NA      NA       NA\n",
       "X1388902_at          NA         NA      NA       NA\n",
       "X1388955_at          NA         NA      NA       NA\n",
       "X1389211_at          NA         NA      NA       NA\n",
       "X1389244_x_at        NA         NA      NA       NA\n",
       "X1389307_at          NA         NA      NA       NA\n",
       "X1389350_at          NA         NA      NA       NA\n",
       "X1389467_at          NA         NA      NA       NA\n",
       "X1389600_at          NA         NA      NA       NA\n",
       "X1389637_at          NA         NA      NA       NA\n",
       "X1389732_at          NA         NA      NA       NA\n",
       "X1389734_x_at        NA         NA      NA       NA\n",
       "X1389781_at          NA         NA      NA       NA\n",
       "X1389966_at          NA         NA      NA       NA\n",
       "X1389975_at          NA         NA      NA       NA\n",
       "X1390112_at          NA         NA      NA       NA\n",
       "X1390196_at          NA         NA      NA       NA\n",
       "X1390209_at          NA         NA      NA       NA\n",
       "X1390238_at          NA         NA      NA       NA\n",
       "X1390326_at          NA         NA      NA       NA\n",
       "X1390472_at          NA         NA      NA       NA\n",
       "X1390493_at          NA         NA      NA       NA\n",
       "X1390591_at          NA         NA      NA       NA\n",
       "X1390659_at          NA         NA      NA       NA\n",
       "X1390776_at          NA         NA      NA       NA\n",
       "X1390795_at          NA         NA      NA       NA\n",
       "X1390807_at          NA         NA      NA       NA\n",
       "X1390896_at          NA         NA      NA       NA\n",
       "X1390951_at          NA         NA      NA       NA\n",
       "X1391013_at          NA         NA      NA       NA\n",
       "X1391059_at          NA         NA      NA       NA\n",
       "X1391068_at          NA         NA      NA       NA\n",
       "X1391092_at          NA         NA      NA       NA\n",
       "X1391187_at          NA         NA      NA       NA\n",
       "X1391293_at          NA         NA      NA       NA\n",
       "X1391397_at          NA         NA      NA       NA\n",
       "X1391417_at          NA         NA      NA       NA\n",
       "X1391429_at          NA         NA      NA       NA\n",
       "X1391509_at          NA         NA      NA       NA\n",
       "X1391547_at          NA         NA      NA       NA\n",
       "X1391624_at          NA         NA      NA       NA\n",
       "X1391653_at          NA         NA      NA       NA\n",
       "X1391656_at          NA         NA      NA       NA\n",
       "X1391791_at          NA         NA      NA       NA\n",
       "X1391806_at          NA         NA      NA       NA\n",
       "X1391856_at          NA         NA      NA       NA\n",
       "X1391884_at          NA         NA      NA       NA\n",
       "X1392003_at          NA         NA      NA       NA\n",
       "X1392090_at          NA         NA      NA       NA\n",
       "X1392166_at          NA         NA      NA       NA\n",
       "X1392308_at          NA         NA      NA       NA\n",
       "X1392322_at          NA         NA      NA       NA\n",
       "X1392384_s_at        NA         NA      NA       NA\n",
       "X1392703_at          NA         NA      NA       NA\n",
       "X1392734_at          NA         NA      NA       NA\n",
       "X1392754_at          NA         NA      NA       NA\n",
       "X1392969_at          NA         NA      NA       NA\n",
       "X1393123_at          NA         NA      NA       NA\n",
       "X1393139_at          NA         NA      NA       NA\n",
       "X1393182_at          NA         NA      NA       NA\n",
       "X1393237_at          NA         NA      NA       NA\n",
       "X1393281_at          NA         NA      NA       NA\n",
       "X1393335_at          NA         NA      NA       NA\n",
       "X1393386_at          NA         NA      NA       NA\n",
       "X1393401_at          NA         NA      NA       NA\n",
       "X1393403_at          NA         NA      NA       NA\n",
       "X1393415_at          NA         NA      NA       NA\n",
       "X1393508_at          NA         NA      NA       NA\n",
       "X1393624_at          NA         NA      NA       NA\n",
       "X1393632_at          NA         NA      NA       NA\n",
       "X1393672_at          NA         NA      NA       NA\n",
       "X1393771_at          NA         NA      NA       NA\n",
       "X1393826_at          NA         NA      NA       NA\n",
       "X1393845_a_at        NA         NA      NA       NA\n",
       "X1393945_at          NA         NA      NA       NA\n",
       "X1394112_at          NA         NA      NA       NA\n",
       "X1394135_at          NA         NA      NA       NA\n",
       "X1394395_at          NA         NA      NA       NA\n",
       "X1394399_at          NA         NA      NA       NA\n",
       "X1394578_at          NA         NA      NA       NA\n",
       "X1394681_at          NA         NA      NA       NA\n",
       "X1394844_s_at        NA         NA      NA       NA\n",
       "X1395041_at          NA         NA      NA       NA\n",
       "X1395255_at          NA         NA      NA       NA\n",
       "X1395335_at          NA         NA      NA       NA\n",
       "X1395381_at          NA         NA      NA       NA\n",
       "X1395403_at          NA         NA      NA       NA\n",
       "X1395528_at          NA         NA      NA       NA\n",
       "X1396040_at          NA         NA      NA       NA\n",
       "X1397227_at          NA         NA      NA       NA\n",
       "X1397537_at          NA         NA      NA       NA\n",
       "X1397700_x_at        NA         NA      NA       NA\n",
       "X1397729_x_at        NA         NA      NA       NA\n",
       "X1397773_at          NA         NA      NA       NA\n",
       "X1397859_x_at        NA         NA      NA       NA\n",
       "X1398258_at          NA         NA      NA       NA\n",
       "X1398260_a_at        NA         NA      NA       NA\n",
       "X1398267_at          NA         NA      NA       NA\n",
       "X1398282_at          NA         NA      NA       NA\n",
       "X1398318_at          NA         NA      NA       NA\n",
       "X1398357_at          NA         NA      NA       NA\n",
       "X1398368_at          NA         NA      NA       NA\n",
       "X1398514_at          NA         NA      NA       NA\n",
       "X1398577_at          NA         NA      NA       NA\n",
       "X1398578_at          NA         NA      NA       NA\n",
       "X1398625_at          NA         NA      NA       NA\n",
       "X1398634_at          NA         NA      NA       NA\n",
       "X1398716_at          NA         NA      NA       NA\n",
       "\n",
       "Residual standard error: NaN on 0 degrees of freedom\n",
       "Multiple R-squared:      1,\tAdjusted R-squared:    NaN \n",
       "F-statistic:   NaN on 41 and 0 DF,  p-value: NA\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mod = lm(y~.,,data=df)\n",
    "summary(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = as.matrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "8.19495738961432e+21"
      ],
      "text/latex": [
       "8.19495738961432e+21"
      ],
      "text/markdown": [
       "8.19495738961432e+21"
      ],
      "text/plain": [
       "[1] 8.194957e+21"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xtx=t(X)%*%X\n",
    "kappa(xtx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression Solution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'glmnet' was built under R version 4.0.4\"\n",
      "Loading required package: Matrix\n",
      "\n",
      "Warning message:\n",
      "\"package 'Matrix' was built under R version 4.0.5\"\n",
      "Loaded glmnet 4.1-1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library('glmnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table width=\"100%\" summary=\"page for glmnet {glmnet}\"><tr><td>glmnet {glmnet}</td><td style=\"text-align: right;\">R Documentation</td></tr></table>\n",
       "\n",
       "<h2>fit a GLM with lasso or elasticnet regularization</h2>\n",
       "\n",
       "<h3>Description</h3>\n",
       "\n",
       "<p>Fit a generalized linear model via penalized maximum likelihood.  The\n",
       "regularization path is computed for the lasso or elasticnet penalty at a\n",
       "grid of values for the regularization parameter lambda. Can deal with all\n",
       "shapes of data, including very large sparse data matrices. Fits linear,\n",
       "logistic and multinomial, poisson, and Cox regression models.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Usage</h3>\n",
       "\n",
       "<pre>\n",
       "glmnet(\n",
       "  x,\n",
       "  y,\n",
       "  family = c(\"gaussian\", \"binomial\", \"poisson\", \"multinomial\", \"cox\", \"mgaussian\"),\n",
       "  weights = NULL,\n",
       "  offset = NULL,\n",
       "  alpha = 1,\n",
       "  nlambda = 100,\n",
       "  lambda.min.ratio = ifelse(nobs &lt; nvars, 0.01, 1e-04),\n",
       "  lambda = NULL,\n",
       "  standardize = TRUE,\n",
       "  intercept = TRUE,\n",
       "  thresh = 1e-07,\n",
       "  dfmax = nvars + 1,\n",
       "  pmax = min(dfmax * 2 + 20, nvars),\n",
       "  exclude = NULL,\n",
       "  penalty.factor = rep(1, nvars),\n",
       "  lower.limits = -Inf,\n",
       "  upper.limits = Inf,\n",
       "  maxit = 1e+05,\n",
       "  type.gaussian = ifelse(nvars &lt; 500, \"covariance\", \"naive\"),\n",
       "  type.logistic = c(\"Newton\", \"modified.Newton\"),\n",
       "  standardize.response = FALSE,\n",
       "  type.multinomial = c(\"ungrouped\", \"grouped\"),\n",
       "  relax = FALSE,\n",
       "  trace.it = 0,\n",
       "  ...\n",
       ")\n",
       "\n",
       "relax.glmnet(fit, x, ..., maxp = n - 3, path = FALSE, check.args = TRUE)\n",
       "</pre>\n",
       "\n",
       "\n",
       "<h3>Arguments</h3>\n",
       "\n",
       "<table summary=\"R argblock\">\n",
       "<tr valign=\"top\"><td><code>x</code></td>\n",
       "<td>\n",
       "<p>input matrix, of dimension nobs x nvars; each row is an observation\n",
       "vector. Can be in sparse matrix format (inherit from class\n",
       "<code>\"sparseMatrix\"</code> as in package <code>Matrix</code>)</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>y</code></td>\n",
       "<td>\n",
       "<p>response variable. Quantitative for <code>family=\"gaussian\"</code>, or\n",
       "<code>family=\"poisson\"</code> (non-negative counts). For <code>family=\"binomial\"</code>\n",
       "should be either a factor with two levels, or a two-column matrix of counts\n",
       "or proportions (the second column is treated as the target class; for a\n",
       "factor, the last level in alphabetical order is the target class). For\n",
       "<code>family=\"multinomial\"</code>, can be a <code>nc&gt;=2</code> level factor, or a matrix\n",
       "with <code>nc</code> columns of counts or proportions. For either\n",
       "<code>\"binomial\"</code> or <code>\"multinomial\"</code>, if <code>y</code> is presented as a\n",
       "vector, it will be coerced into a factor. For <code>family=\"cox\"</code>, preferably\n",
       "a <code>Surv</code> object from the survival package: see Details section for\n",
       "more information. For <code>family=\"mgaussian\"</code>, <code>y</code> is a matrix\n",
       "of quantitative responses.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>family</code></td>\n",
       "<td>\n",
       "<p>Either a character string representing\n",
       "one of the built-in families, or else a <code>glm()</code> family object. For more\n",
       "information, see Details section below or the documentation for response\n",
       "type (above).</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>weights</code></td>\n",
       "<td>\n",
       "<p>observation weights. Can be total counts if responses are\n",
       "proportion matrices. Default is 1 for each observation</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>offset</code></td>\n",
       "<td>\n",
       "<p>A vector of length <code>nobs</code> that is included in the linear\n",
       "predictor (a <code>nobs x nc</code> matrix for the <code>\"multinomial\"</code> family).\n",
       "Useful for the <code>\"poisson\"</code> family (e.g. log of exposure time), or for\n",
       "refining a model by starting at a current fit. Default is <code>NULL</code>. If\n",
       "supplied, then values must also be supplied to the <code>predict</code> function.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>alpha</code></td>\n",
       "<td>\n",
       "<p>The elasticnet mixing parameter, with <i>0&le;&alpha;&le; 1</i>.\n",
       "The penalty is defined as\n",
       "</p>\n",
       "<p style=\"text-align: center;\"><i>(1-&alpha;)/2||&beta;||_2^2+&alpha;||&beta;||_1.</i></p>\n",
       " <p><code>alpha=1</code> is the\n",
       "lasso penalty, and <code>alpha=0</code> the ridge penalty.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>nlambda</code></td>\n",
       "<td>\n",
       "<p>The number of <code>lambda</code> values - default is 100.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>lambda.min.ratio</code></td>\n",
       "<td>\n",
       "<p>Smallest value for <code>lambda</code>, as a fraction of\n",
       "<code>lambda.max</code>, the (data derived) entry value (i.e. the smallest value\n",
       "for which all coefficients are zero). The default depends on the sample size\n",
       "<code>nobs</code> relative to the number of variables <code>nvars</code>. If <code>nobs\n",
       "&gt; nvars</code>, the default is <code>0.0001</code>, close to zero.  If <code>nobs &lt;\n",
       "nvars</code>, the default is <code>0.01</code>.  A very small value of\n",
       "<code>lambda.min.ratio</code> will lead to a saturated fit in the <code>nobs &lt;\n",
       "nvars</code> case. This is undefined for <code>\"binomial\"</code> and\n",
       "<code>\"multinomial\"</code> models, and <code>glmnet</code> will exit gracefully when the\n",
       "percentage deviance explained is almost 1.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>lambda</code></td>\n",
       "<td>\n",
       "<p>A user supplied <code>lambda</code> sequence. Typical usage is to\n",
       "have the program compute its own <code>lambda</code> sequence based on\n",
       "<code>nlambda</code> and <code>lambda.min.ratio</code>. Supplying a value of\n",
       "<code>lambda</code> overrides this. WARNING: use with care. Avoid supplying a\n",
       "single value for <code>lambda</code> (for predictions after CV use\n",
       "<code>predict()</code> instead).  Supply instead a decreasing sequence of\n",
       "<code>lambda</code> values. <code>glmnet</code> relies on its warms starts for speed,\n",
       "and its often faster to fit a whole path than compute a single fit.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>standardize</code></td>\n",
       "<td>\n",
       "<p>Logical flag for x variable standardization, prior to\n",
       "fitting the model sequence. The coefficients are always returned on the\n",
       "original scale. Default is <code>standardize=TRUE</code>.  If variables are in the\n",
       "same units already, you might not wish to standardize. See details below for\n",
       "y standardization with <code>family=\"gaussian\"</code>.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>intercept</code></td>\n",
       "<td>\n",
       "<p>Should intercept(s) be fitted (default=TRUE) or set to zero\n",
       "(FALSE)</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>thresh</code></td>\n",
       "<td>\n",
       "<p>Convergence threshold for coordinate descent. Each inner\n",
       "coordinate-descent loop continues until the maximum change in the objective\n",
       "after any coefficient update is less than <code>thresh</code> times the null\n",
       "deviance. Defaults value is <code>1E-7</code>.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>dfmax</code></td>\n",
       "<td>\n",
       "<p>Limit the maximum number of variables in the model. Useful for\n",
       "very large <code>nvars</code>, if a partial path is desired.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>pmax</code></td>\n",
       "<td>\n",
       "<p>Limit the maximum number of variables ever to be nonzero</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>exclude</code></td>\n",
       "<td>\n",
       "<p>Indices of variables to be excluded from the model. Default\n",
       "is none. Equivalent to an infinite penalty factor (next item).</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>penalty.factor</code></td>\n",
       "<td>\n",
       "<p>Separate penalty factors can be applied to each\n",
       "coefficient. This is a number that multiplies <code>lambda</code> to allow\n",
       "differential shrinkage. Can be 0 for some variables, which implies no\n",
       "shrinkage, and that variable is always included in the model. Default is 1\n",
       "for all variables (and implicitly infinity for variables listed in\n",
       "<code>exclude</code>). Note: the penalty factors are internally rescaled to sum to\n",
       "nvars, and the lambda sequence will reflect this change.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>lower.limits</code></td>\n",
       "<td>\n",
       "<p>Vector of lower limits for each coefficient; default\n",
       "<code>-Inf</code>. Each of these must be non-positive. Can be presented as a\n",
       "single value (which will then be replicated), else a vector of length\n",
       "<code>nvars</code></p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>upper.limits</code></td>\n",
       "<td>\n",
       "<p>Vector of upper limits for each coefficient; default\n",
       "<code>Inf</code>. See <code>lower.limits</code></p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>maxit</code></td>\n",
       "<td>\n",
       "<p>Maximum number of passes over the data for all lambda values;\n",
       "default is 10^5.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>type.gaussian</code></td>\n",
       "<td>\n",
       "<p>Two algorithm types are supported for (only)\n",
       "<code>family=\"gaussian\"</code>. The default when <code>nvar&lt;500</code> is\n",
       "<code>type.gaussian=\"covariance\"</code>, and saves all inner-products ever\n",
       "computed. This can be much faster than <code>type.gaussian=\"naive\"</code>, which\n",
       "loops through <code>nobs</code> every time an inner-product is computed. The\n",
       "latter can be far more efficient for <code>nvar &gt;&gt; nobs</code> situations, or when\n",
       "<code>nvar &gt; 500</code>.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>type.logistic</code></td>\n",
       "<td>\n",
       "<p>If <code>\"Newton\"</code> then the exact hessian is used\n",
       "(default), while <code>\"modified.Newton\"</code> uses an upper-bound on the\n",
       "hessian, and can be faster.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>standardize.response</code></td>\n",
       "<td>\n",
       "<p>This is for the <code>family=\"mgaussian\"</code>\n",
       "family, and allows the user to standardize the response variables</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>type.multinomial</code></td>\n",
       "<td>\n",
       "<p>If <code>\"grouped\"</code> then a grouped lasso penalty is\n",
       "used on the multinomial coefficients for a variable. This ensures they are\n",
       "all in our out together. The default is <code>\"ungrouped\"</code></p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>relax</code></td>\n",
       "<td>\n",
       "<p>If <code>TRUE</code> then for each <em>active set</em> in the path of\n",
       "solutions, the model is refit without any regularization. See <code>details</code>\n",
       "for more information. This argument is new, and users may experience convergence issues\n",
       "with small datasets, especially with non-gaussian families. Limiting the\n",
       "value of 'maxp' can alleviate these issues in some cases.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>trace.it</code></td>\n",
       "<td>\n",
       "<p>If <code>trace.it=1</code>, then a progress bar is displayed;\n",
       "useful for big models that take a long time to fit.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>...</code></td>\n",
       "<td>\n",
       "<p>Additional argument used in <code>relax.glmnet</code>. These include\n",
       "some of the original arguments to 'glmnet', and each must be named if used.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>fit</code></td>\n",
       "<td>\n",
       "<p>For <code>relax.glmnet</code> a fitted 'glmnet' object</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>maxp</code></td>\n",
       "<td>\n",
       "<p>a limit on how many relaxed coefficients are allowed. Default is\n",
       "'n-3', where 'n' is the sample size. This may not be sufficient for\n",
       "non-gaussian familes, in which case users should supply a smaller value.\n",
       "This argument can be supplied directly to 'glmnet'.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>path</code></td>\n",
       "<td>\n",
       "<p>Since <code>glmnet</code> does not do stepsize optimization, the Newton\n",
       "algorithm can get stuck and not converge, especially with relaxed fits. With <code>path=TRUE</code>,\n",
       "each relaxed fit on a particular set of variables is computed pathwise using the original sequence\n",
       "of lambda values (with a zero attached to the end). Not needed for Gaussian models, and should not\n",
       "be used unless needed, since will lead to longer compute times. Default is <code>path=FALSE</code>.\n",
       "appropriate subset of variables</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>check.args</code></td>\n",
       "<td>\n",
       "<p>Should <code>relax.glmnet</code> make sure that all the data\n",
       "dependent arguments used in creating 'fit' have been resupplied. Default is\n",
       "'TRUE'.</p>\n",
       "</td></tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "<h3>Details</h3>\n",
       "\n",
       "<p>The sequence of models implied by <code>lambda</code> is fit by coordinate\n",
       "descent. For <code>family=\"gaussian\"</code> this is the lasso sequence if\n",
       "<code>alpha=1</code>, else it is the elasticnet sequence.\n",
       "</p>\n",
       "<p>The objective function for <code>\"gaussian\"</code> is </p>\n",
       "<p style=\"text-align: center;\"><i>1/2 RSS/nobs +\n",
       "&lambda;*penalty,</i></p>\n",
       "<p> and for the other models it is </p>\n",
       "<p style=\"text-align: center;\"><i>-loglik/nobs +\n",
       "&lambda;*penalty.</i></p>\n",
       "<p> Note also that for <code>\"gaussian\"</code>, <code>glmnet</code>\n",
       "standardizes y to have unit variance (using 1/n rather than 1/(n-1) formula)\n",
       "before computing its lambda sequence (and then unstandardizes the resulting\n",
       "coefficients); if you wish to reproduce/compare results with other software,\n",
       "best to supply a standardized y. The coefficients for any predictor\n",
       "variables with zero variance are set to zero for all values of lambda.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h4>Details on <code>family</code> option</h4>\n",
       "\n",
       "<p>From version 4.0 onwards, glmnet supports both the original built-in families,\n",
       "as well as <em>any</em> family object as used by <code>stats:glm()</code>.\n",
       "This opens the door to a wide variety of additional models. For example\n",
       "<code>family=binomial(link=cloglog)</code> or <code>family=negative.binomial(theta=1.5)</code> (from the MASS library).\n",
       "Note that the code runs faster for the built-in families.\n",
       "</p>\n",
       "<p>The built in families are specifed via a character string. For all families,\n",
       "the object produced is a lasso or elasticnet regularization path for fitting the\n",
       "generalized linear regression paths, by maximizing the appropriate penalized\n",
       "log-likelihood (partial likelihood for the &quot;cox&quot; model). Sometimes the\n",
       "sequence is truncated before <code>nlambda</code> values of <code>lambda</code> have\n",
       "been used, because of instabilities in the inverse link functions near a\n",
       "saturated fit. <code>glmnet(...,family=\"binomial\")</code> fits a traditional\n",
       "logistic regression model for the log-odds.\n",
       "<code>glmnet(...,family=\"multinomial\")</code> fits a symmetric multinomial model,\n",
       "where each class is represented by a linear model (on the log-scale). The\n",
       "penalties take care of redundancies. A two-class <code>\"multinomial\"</code> model\n",
       "will produce the same fit as the corresponding <code>\"binomial\"</code> model,\n",
       "except the pair of coefficient matrices will be equal in magnitude and\n",
       "opposite in sign, and half the <code>\"binomial\"</code> values.\n",
       "Two useful additional families are the <code>family=\"mgaussian\"</code> family and\n",
       "the <code>type.multinomial=\"grouped\"</code> option for multinomial fitting. The\n",
       "former allows a multi-response gaussian model to be fit, using a &quot;group\n",
       "-lasso&quot; penalty on the coefficients for each variable. Tying the responses\n",
       "together like this is called &quot;multi-task&quot; learning in some domains. The\n",
       "grouped multinomial allows the same penalty for the\n",
       "<code>family=\"multinomial\"</code> model, which is also multi-responsed. For both\n",
       "of these the penalty on the coefficient vector for variable j is\n",
       "</p>\n",
       "<p style=\"text-align: center;\"><i>(1-&alpha;)/2||&beta;_j||_2^2+&alpha;||&beta;_j||_2.</i></p>\n",
       "<p> When <code>alpha=1</code>\n",
       "this is a group-lasso penalty, and otherwise it mixes with quadratic just\n",
       "like elasticnet. A small detail in the Cox model: if death times are tied\n",
       "with censored times, we assume the censored times occurred just\n",
       "<em>before</em> the death times in computing the Breslow approximation; if\n",
       "users prefer the usual convention of <em>after</em>, they can add a small\n",
       "number to all censoring times to achieve this effect.\n",
       "</p>\n",
       "\n",
       "\n",
       "\n",
       "<h4>Details on response for <code>family=\"cox\"</code></h4>\n",
       "\n",
       "<p>For Cox models, the response should preferably be a <code>Surv</code> object,\n",
       "created by the <code>Surv()</code> function in <span class=\"pkg\">survival</span> package. For\n",
       "right-censored data, this object should have type &quot;right&quot;, and for\n",
       "(start, stop] data, it should have type &quot;counting&quot;. To fit stratified Cox\n",
       "models, strata should be added to the response via the <code>stratifySurv()</code>\n",
       "function before passing the response to <code>glmnet()</code>. (For backward\n",
       "compatibility, right-censored data can also be passed as a\n",
       "two-column matrix with columns named 'time' and 'status'. The\n",
       "latter is a binary variable, with '1' indicating death, and '0' indicating\n",
       "right censored.)\n",
       "</p>\n",
       "\n",
       "\n",
       "\n",
       "<h4>Details on <code>relax</code> option</h4>\n",
       "\n",
       "<p>If <code>relax=TRUE</code>\n",
       "a duplicate sequence of models is produced, where each active set in the\n",
       "elastic-net path is refit without regularization. The result of this is a\n",
       "matching <code>\"glmnet\"</code> object which is stored on the original object in a\n",
       "component named <code>\"relaxed\"</code>, and is part of the glmnet output.\n",
       "Generally users will not call <code>relax.glmnet</code> directly, unless the\n",
       "original 'glmnet' object took a long time to fit. But if they do, they must\n",
       "supply the fit, and all the original arguments used to create that fit. They\n",
       "can limit the length of the relaxed path via 'maxp'.\n",
       "</p>\n",
       "\n",
       "\n",
       "\n",
       "<h3>Value</h3>\n",
       "\n",
       "<p>An object with S3 class <code>\"glmnet\",\"*\" </code>, where <code>\"*\"</code> is\n",
       "<code>\"elnet\"</code>, <code>\"lognet\"</code>, <code>\"multnet\"</code>, <code>\"fishnet\"</code>\n",
       "(poisson), <code>\"coxnet\"</code> or <code>\"mrelnet\"</code> for the various types of\n",
       "models. If the model was created with <code>relax=TRUE</code> then this class has\n",
       "a prefix class of <code>\"relaxed\"</code>.  </p>\n",
       "<table summary=\"R valueblock\">\n",
       "<tr valign=\"top\"><td><code>call</code></td>\n",
       "<td>\n",
       "<p>the call that produced this\n",
       "object</p>\n",
       "</td></tr> <tr valign=\"top\"><td><code>a0</code></td>\n",
       "<td>\n",
       "<p>Intercept sequence of length <code>length(lambda)</code></p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>beta</code></td>\n",
       "<td>\n",
       "<p>For <code>\"elnet\"</code>, <code>\"lognet\"</code>, <code>\"fishnet\"</code> and\n",
       "<code>\"coxnet\"</code> models, a <code>nvars x length(lambda)</code> matrix of\n",
       "coefficients, stored in sparse column format (<code>\"CsparseMatrix\"</code>). For\n",
       "<code>\"multnet\"</code> and <code>\"mgaussian\"</code>, a list of <code>nc</code> such matrices,\n",
       "one for each class.</p>\n",
       "</td></tr> <tr valign=\"top\"><td><code>lambda</code></td>\n",
       "<td>\n",
       "<p>The actual sequence of <code>lambda</code>\n",
       "values used. When <code>alpha=0</code>, the largest lambda reported does not quite\n",
       "give the zero coefficients reported (<code>lambda=inf</code> would in principle).\n",
       "Instead, the largest <code>lambda</code> for <code>alpha=0.001</code> is used, and the\n",
       "sequence of <code>lambda</code> values is derived from this.</p>\n",
       "</td></tr> <tr valign=\"top\"><td><code>dev.ratio</code></td>\n",
       "<td>\n",
       "<p>The\n",
       "fraction of (null) deviance explained (for <code>\"elnet\"</code>, this is the\n",
       "R-square). The deviance calculations incorporate weights if present in the\n",
       "model. The deviance is defined to be 2*(loglike_sat - loglike), where\n",
       "loglike_sat is the log-likelihood for the saturated model (a model with a\n",
       "free parameter per observation). Hence dev.ratio=1-dev/nulldev.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>nulldev</code></td>\n",
       "<td>\n",
       "<p>Null deviance (per observation). This is defined to be\n",
       "2*(loglike_sat -loglike(Null)); The NULL model refers to the intercept\n",
       "model, except for the Cox, where it is the 0 model.</p>\n",
       "</td></tr> <tr valign=\"top\"><td><code>df</code></td>\n",
       "<td>\n",
       "<p>The number of\n",
       "nonzero coefficients for each value of <code>lambda</code>. For <code>\"multnet\"</code>,\n",
       "this is the number of variables with a nonzero coefficient for <em>any</em>\n",
       "class.</p>\n",
       "</td></tr> <tr valign=\"top\"><td><code>dfmat</code></td>\n",
       "<td>\n",
       "<p>For <code>\"multnet\"</code> and <code>\"mrelnet\"</code> only. A\n",
       "matrix consisting of the number of nonzero coefficients per class</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>dim</code></td>\n",
       "<td>\n",
       "<p>dimension of coefficient matrix (ices)</p>\n",
       "</td></tr> <tr valign=\"top\"><td><code>nobs</code></td>\n",
       "<td>\n",
       "<p>number of\n",
       "observations</p>\n",
       "</td></tr> <tr valign=\"top\"><td><code>npasses</code></td>\n",
       "<td>\n",
       "<p>total passes over the data summed over all\n",
       "lambda values</p>\n",
       "</td></tr> <tr valign=\"top\"><td><code>offset</code></td>\n",
       "<td>\n",
       "<p>a logical variable indicating whether an offset\n",
       "was included in the model</p>\n",
       "</td></tr> <tr valign=\"top\"><td><code>jerr</code></td>\n",
       "<td>\n",
       "<p>error flag, for warnings and errors\n",
       "(largely for internal debugging).</p>\n",
       "</td></tr> <tr valign=\"top\"><td><code>relaxed</code></td>\n",
       "<td>\n",
       "<p>If <code>relax=TRUE</code>, this\n",
       "additional item is another glmnet object with different values for\n",
       "<code>beta</code> and <code>dev.ratio</code></p>\n",
       "</td></tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "<h3>Author(s)</h3>\n",
       "\n",
       "<p>Jerome Friedman, Trevor Hastie, Balasubramanian Narasimhan, Noah\n",
       "Simon, Kenneth Tay and Rob Tibshirani<br /> Maintainer: Trevor Hastie\n",
       "<a href=\"mailto:hastie@stanford.edu\">hastie@stanford.edu</a>\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>References</h3>\n",
       "\n",
       "<p>Friedman, J., Hastie, T. and Tibshirani, R. (2008)\n",
       "<em>Regularization Paths for Generalized Linear Models via Coordinate\n",
       "Descent (2010), Journal of Statistical Software, Vol. 33(1), 1-22</em>,\n",
       "<a href=\"https://web.stanford.edu/~hastie/Papers/glmnet.pdf\">https://web.stanford.edu/~hastie/Papers/glmnet.pdf</a>.<br />\n",
       "Simon, N., Friedman, J., Hastie, T. and Tibshirani, R. (2011)\n",
       "<em>Regularization Paths for Cox's Proportional\n",
       "Hazards Model via Coordinate Descent, Journal of Statistical Software, Vol.\n",
       "39(5), 1-13</em>, <a href=\"https://www.jstatsoft.org/v39/i05/\">https://www.jstatsoft.org/v39/i05/</a>.<br /> Tibshirani,\n",
       "Robert, Bien, J., Friedman, J., Hastie, T.,Simon, N.,Taylor, J. and\n",
       "Tibshirani, Ryan. (2012) <em>Strong Rules for Discarding Predictors in\n",
       "Lasso-type Problems, JRSSB, Vol. 74(2), 245-266</em>,\n",
       "<a href=\"https://statweb.stanford.edu/~tibs/ftp/strong.pdf\">https://statweb.stanford.edu/~tibs/ftp/strong.pdf</a>.<br />\n",
       "Hastie, T., Tibshirani, Robert and Tibshirani, Ryan. <em>Extended\n",
       "Comparisons of Best Subset Selection, Forward Stepwise Selection, and the\n",
       "Lasso (2017), Stanford Statistics Technical Report</em>,\n",
       "<a href=\"https://arxiv.org/abs/1707.08692\">https://arxiv.org/abs/1707.08692</a>.<br />\n",
       "Glmnet webpage with four vignettes, <a href=\"https://glmnet.stanford.edu\">https://glmnet.stanford.edu</a>.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>See Also</h3>\n",
       "\n",
       "<p><code>print</code>, <code>predict</code>, <code>coef</code> and <code>plot</code> methods,\n",
       "and the <code>cv.glmnet</code> function.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Examples</h3>\n",
       "\n",
       "<pre>\n",
       "\n",
       "# Gaussian\n",
       "x = matrix(rnorm(100 * 20), 100, 20)\n",
       "y = rnorm(100)\n",
       "fit1 = glmnet(x, y)\n",
       "print(fit1)\n",
       "coef(fit1, s = 0.01)  # extract coefficients at a single value of lambda\n",
       "predict(fit1, newx = x[1:10, ], s = c(0.01, 0.005))  # make predictions\n",
       "\n",
       "# Relaxed\n",
       "fit1r = glmnet(x, y, relax = TRUE)  # can be used with any model\n",
       "\n",
       "# multivariate gaussian\n",
       "y = matrix(rnorm(100 * 3), 100, 3)\n",
       "fit1m = glmnet(x, y, family = \"mgaussian\")\n",
       "plot(fit1m, type.coef = \"2norm\")\n",
       "\n",
       "# binomial\n",
       "g2 = sample(c(0,1), 100, replace = TRUE)\n",
       "fit2 = glmnet(x, g2, family = \"binomial\")\n",
       "fit2n = glmnet(x, g2, family = binomial(link=cloglog))\n",
       "fit2r = glmnet(x,g2, family = \"binomial\", relax=TRUE)\n",
       "fit2rp = glmnet(x,g2, family = \"binomial\", relax=TRUE, path=TRUE)\n",
       "\n",
       "# multinomial\n",
       "g4 = sample(1:4, 100, replace = TRUE)\n",
       "fit3 = glmnet(x, g4, family = \"multinomial\")\n",
       "fit3a = glmnet(x, g4, family = \"multinomial\", type.multinomial = \"grouped\")\n",
       "# poisson\n",
       "N = 500\n",
       "p = 20\n",
       "nzc = 5\n",
       "x = matrix(rnorm(N * p), N, p)\n",
       "beta = rnorm(nzc)\n",
       "f = x[, seq(nzc)] %*% beta\n",
       "mu = exp(f)\n",
       "y = rpois(N, mu)\n",
       "fit = glmnet(x, y, family = \"poisson\")\n",
       "plot(fit)\n",
       "pfit = predict(fit, x, s = 0.001, type = \"response\")\n",
       "plot(pfit, y)\n",
       "\n",
       "# Cox\n",
       "set.seed(10101)\n",
       "N = 1000\n",
       "p = 30\n",
       "nzc = p/3\n",
       "x = matrix(rnorm(N * p), N, p)\n",
       "beta = rnorm(nzc)\n",
       "fx = x[, seq(nzc)] %*% beta/3\n",
       "hx = exp(fx)\n",
       "ty = rexp(N, hx)\n",
       "tcens = rbinom(n = N, prob = 0.3, size = 1)  # censoring indicator\n",
       "y = cbind(time = ty, status = 1 - tcens)  # y=Surv(ty,1-tcens) with library(survival)\n",
       "fit = glmnet(x, y, family = \"cox\")\n",
       "plot(fit)\n",
       "\n",
       "# Cox example with (start, stop] data\n",
       "set.seed(2)\n",
       "nobs &lt;- 100; nvars &lt;- 15\n",
       "xvec &lt;- rnorm(nobs * nvars)\n",
       "xvec[sample.int(nobs * nvars, size = 0.4 * nobs * nvars)] &lt;- 0\n",
       "x &lt;- matrix(xvec, nrow = nobs)\n",
       "start_time &lt;- runif(100, min = 0, max = 5)\n",
       "stop_time &lt;- start_time + runif(100, min = 0.1, max = 3)\n",
       "status &lt;- rbinom(n = nobs, prob = 0.3, size = 1)\n",
       "jsurv_ss &lt;- survival::Surv(start_time, stop_time, status)\n",
       "fit &lt;- glmnet(x, jsurv_ss, family = \"cox\")\n",
       "\n",
       "# Cox example with strata\n",
       "jsurv_ss2 &lt;- stratifySurv(jsurv_ss, rep(1:2, each = 50))\n",
       "fit &lt;- glmnet(x, jsurv_ss2, family = \"cox\")\n",
       "\n",
       "# Sparse\n",
       "n = 10000\n",
       "p = 200\n",
       "nzc = trunc(p/10)\n",
       "x = matrix(rnorm(n * p), n, p)\n",
       "iz = sample(1:(n * p), size = n * p * 0.85, replace = FALSE)\n",
       "x[iz] = 0\n",
       "sx = Matrix(x, sparse = TRUE)\n",
       "inherits(sx, \"sparseMatrix\")  #confirm that it is sparse\n",
       "beta = rnorm(nzc)\n",
       "fx = x[, seq(nzc)] %*% beta\n",
       "eps = rnorm(n)\n",
       "y = fx + eps\n",
       "px = exp(fx)\n",
       "px = px/(1 + px)\n",
       "ly = rbinom(n = length(px), prob = px, size = 1)\n",
       "system.time(fit1 &lt;- glmnet(sx, y))\n",
       "system.time(fit2n &lt;- glmnet(x, y))\n",
       "\n",
       "</pre>\n",
       "\n",
       "<hr /><div style=\"text-align: center;\">[Package <em>glmnet</em> version 4.1-1 ]</div>"
      ],
      "text/latex": [
       "\\inputencoding{utf8}\n",
       "\\HeaderA{glmnet}{fit a GLM with lasso or elasticnet regularization}{glmnet}\n",
       "\\aliasA{relax.glmnet}{glmnet}{relax.glmnet}\n",
       "\\keyword{models}{glmnet}\n",
       "\\keyword{regression}{glmnet}\n",
       "%\n",
       "\\begin{Description}\\relax\n",
       "Fit a generalized linear model via penalized maximum likelihood.  The\n",
       "regularization path is computed for the lasso or elasticnet penalty at a\n",
       "grid of values for the regularization parameter lambda. Can deal with all\n",
       "shapes of data, including very large sparse data matrices. Fits linear,\n",
       "logistic and multinomial, poisson, and Cox regression models.\n",
       "\\end{Description}\n",
       "%\n",
       "\\begin{Usage}\n",
       "\\begin{verbatim}\n",
       "glmnet(\n",
       "  x,\n",
       "  y,\n",
       "  family = c(\"gaussian\", \"binomial\", \"poisson\", \"multinomial\", \"cox\", \"mgaussian\"),\n",
       "  weights = NULL,\n",
       "  offset = NULL,\n",
       "  alpha = 1,\n",
       "  nlambda = 100,\n",
       "  lambda.min.ratio = ifelse(nobs < nvars, 0.01, 1e-04),\n",
       "  lambda = NULL,\n",
       "  standardize = TRUE,\n",
       "  intercept = TRUE,\n",
       "  thresh = 1e-07,\n",
       "  dfmax = nvars + 1,\n",
       "  pmax = min(dfmax * 2 + 20, nvars),\n",
       "  exclude = NULL,\n",
       "  penalty.factor = rep(1, nvars),\n",
       "  lower.limits = -Inf,\n",
       "  upper.limits = Inf,\n",
       "  maxit = 1e+05,\n",
       "  type.gaussian = ifelse(nvars < 500, \"covariance\", \"naive\"),\n",
       "  type.logistic = c(\"Newton\", \"modified.Newton\"),\n",
       "  standardize.response = FALSE,\n",
       "  type.multinomial = c(\"ungrouped\", \"grouped\"),\n",
       "  relax = FALSE,\n",
       "  trace.it = 0,\n",
       "  ...\n",
       ")\n",
       "\n",
       "relax.glmnet(fit, x, ..., maxp = n - 3, path = FALSE, check.args = TRUE)\n",
       "\\end{verbatim}\n",
       "\\end{Usage}\n",
       "%\n",
       "\\begin{Arguments}\n",
       "\\begin{ldescription}\n",
       "\\item[\\code{x}] input matrix, of dimension nobs x nvars; each row is an observation\n",
       "vector. Can be in sparse matrix format (inherit from class\n",
       "\\code{\"sparseMatrix\"} as in package \\code{Matrix})\n",
       "\n",
       "\\item[\\code{y}] response variable. Quantitative for \\code{family=\"gaussian\"}, or\n",
       "\\code{family=\"poisson\"} (non-negative counts). For \\code{family=\"binomial\"}\n",
       "should be either a factor with two levels, or a two-column matrix of counts\n",
       "or proportions (the second column is treated as the target class; for a\n",
       "factor, the last level in alphabetical order is the target class). For\n",
       "\\code{family=\"multinomial\"}, can be a \\code{nc>=2} level factor, or a matrix\n",
       "with \\code{nc} columns of counts or proportions. For either\n",
       "\\code{\"binomial\"} or \\code{\"multinomial\"}, if \\code{y} is presented as a\n",
       "vector, it will be coerced into a factor. For \\code{family=\"cox\"}, preferably\n",
       "a \\code{Surv} object from the survival package: see Details section for\n",
       "more information. For \\code{family=\"mgaussian\"}, \\code{y} is a matrix\n",
       "of quantitative responses.\n",
       "\n",
       "\\item[\\code{family}] Either a character string representing\n",
       "one of the built-in families, or else a \\code{glm()} family object. For more\n",
       "information, see Details section below or the documentation for response\n",
       "type (above).\n",
       "\n",
       "\\item[\\code{weights}] observation weights. Can be total counts if responses are\n",
       "proportion matrices. Default is 1 for each observation\n",
       "\n",
       "\\item[\\code{offset}] A vector of length \\code{nobs} that is included in the linear\n",
       "predictor (a \\code{nobs x nc} matrix for the \\code{\"multinomial\"} family).\n",
       "Useful for the \\code{\"poisson\"} family (e.g. log of exposure time), or for\n",
       "refining a model by starting at a current fit. Default is \\code{NULL}. If\n",
       "supplied, then values must also be supplied to the \\code{predict} function.\n",
       "\n",
       "\\item[\\code{alpha}] The elasticnet mixing parameter, with \\eqn{0\\le\\alpha\\le 1}{}.\n",
       "The penalty is defined as\n",
       "\\deqn{(1-\\alpha)/2||\\beta||_2^2+\\alpha||\\beta||_1.}{} \\code{alpha=1} is the\n",
       "lasso penalty, and \\code{alpha=0} the ridge penalty.\n",
       "\n",
       "\\item[\\code{nlambda}] The number of \\code{lambda} values - default is 100.\n",
       "\n",
       "\\item[\\code{lambda.min.ratio}] Smallest value for \\code{lambda}, as a fraction of\n",
       "\\code{lambda.max}, the (data derived) entry value (i.e. the smallest value\n",
       "for which all coefficients are zero). The default depends on the sample size\n",
       "\\code{nobs} relative to the number of variables \\code{nvars}. If \\code{nobs\n",
       "> nvars}, the default is \\code{0.0001}, close to zero.  If \\code{nobs <\n",
       "nvars}, the default is \\code{0.01}.  A very small value of\n",
       "\\code{lambda.min.ratio} will lead to a saturated fit in the \\code{nobs <\n",
       "nvars} case. This is undefined for \\code{\"binomial\"} and\n",
       "\\code{\"multinomial\"} models, and \\code{glmnet} will exit gracefully when the\n",
       "percentage deviance explained is almost 1.\n",
       "\n",
       "\\item[\\code{lambda}] A user supplied \\code{lambda} sequence. Typical usage is to\n",
       "have the program compute its own \\code{lambda} sequence based on\n",
       "\\code{nlambda} and \\code{lambda.min.ratio}. Supplying a value of\n",
       "\\code{lambda} overrides this. WARNING: use with care. Avoid supplying a\n",
       "single value for \\code{lambda} (for predictions after CV use\n",
       "\\code{predict()} instead).  Supply instead a decreasing sequence of\n",
       "\\code{lambda} values. \\code{glmnet} relies on its warms starts for speed,\n",
       "and its often faster to fit a whole path than compute a single fit.\n",
       "\n",
       "\\item[\\code{standardize}] Logical flag for x variable standardization, prior to\n",
       "fitting the model sequence. The coefficients are always returned on the\n",
       "original scale. Default is \\code{standardize=TRUE}.  If variables are in the\n",
       "same units already, you might not wish to standardize. See details below for\n",
       "y standardization with \\code{family=\"gaussian\"}.\n",
       "\n",
       "\\item[\\code{intercept}] Should intercept(s) be fitted (default=TRUE) or set to zero\n",
       "(FALSE)\n",
       "\n",
       "\\item[\\code{thresh}] Convergence threshold for coordinate descent. Each inner\n",
       "coordinate-descent loop continues until the maximum change in the objective\n",
       "after any coefficient update is less than \\code{thresh} times the null\n",
       "deviance. Defaults value is \\code{1E-7}.\n",
       "\n",
       "\\item[\\code{dfmax}] Limit the maximum number of variables in the model. Useful for\n",
       "very large \\code{nvars}, if a partial path is desired.\n",
       "\n",
       "\\item[\\code{pmax}] Limit the maximum number of variables ever to be nonzero\n",
       "\n",
       "\\item[\\code{exclude}] Indices of variables to be excluded from the model. Default\n",
       "is none. Equivalent to an infinite penalty factor (next item).\n",
       "\n",
       "\\item[\\code{penalty.factor}] Separate penalty factors can be applied to each\n",
       "coefficient. This is a number that multiplies \\code{lambda} to allow\n",
       "differential shrinkage. Can be 0 for some variables, which implies no\n",
       "shrinkage, and that variable is always included in the model. Default is 1\n",
       "for all variables (and implicitly infinity for variables listed in\n",
       "\\code{exclude}). Note: the penalty factors are internally rescaled to sum to\n",
       "nvars, and the lambda sequence will reflect this change.\n",
       "\n",
       "\\item[\\code{lower.limits}] Vector of lower limits for each coefficient; default\n",
       "\\code{-Inf}. Each of these must be non-positive. Can be presented as a\n",
       "single value (which will then be replicated), else a vector of length\n",
       "\\code{nvars}\n",
       "\n",
       "\\item[\\code{upper.limits}] Vector of upper limits for each coefficient; default\n",
       "\\code{Inf}. See \\code{lower.limits}\n",
       "\n",
       "\\item[\\code{maxit}] Maximum number of passes over the data for all lambda values;\n",
       "default is 10\\textasciicircum{}5.\n",
       "\n",
       "\\item[\\code{type.gaussian}] Two algorithm types are supported for (only)\n",
       "\\code{family=\"gaussian\"}. The default when \\code{nvar<500} is\n",
       "\\code{type.gaussian=\"covariance\"}, and saves all inner-products ever\n",
       "computed. This can be much faster than \\code{type.gaussian=\"naive\"}, which\n",
       "loops through \\code{nobs} every time an inner-product is computed. The\n",
       "latter can be far more efficient for \\code{nvar >{}> nobs} situations, or when\n",
       "\\code{nvar > 500}.\n",
       "\n",
       "\\item[\\code{type.logistic}] If \\code{\"Newton\"} then the exact hessian is used\n",
       "(default), while \\code{\"modified.Newton\"} uses an upper-bound on the\n",
       "hessian, and can be faster.\n",
       "\n",
       "\\item[\\code{standardize.response}] This is for the \\code{family=\"mgaussian\"}\n",
       "family, and allows the user to standardize the response variables\n",
       "\n",
       "\\item[\\code{type.multinomial}] If \\code{\"grouped\"} then a grouped lasso penalty is\n",
       "used on the multinomial coefficients for a variable. This ensures they are\n",
       "all in our out together. The default is \\code{\"ungrouped\"}\n",
       "\n",
       "\\item[\\code{relax}] If \\code{TRUE} then for each \\emph{active set} in the path of\n",
       "solutions, the model is refit without any regularization. See \\code{details}\n",
       "for more information. This argument is new, and users may experience convergence issues\n",
       "with small datasets, especially with non-gaussian families. Limiting the\n",
       "value of 'maxp' can alleviate these issues in some cases.\n",
       "\n",
       "\\item[\\code{trace.it}] If \\code{trace.it=1}, then a progress bar is displayed;\n",
       "useful for big models that take a long time to fit.\n",
       "\n",
       "\\item[\\code{...}] Additional argument used in \\code{relax.glmnet}. These include\n",
       "some of the original arguments to 'glmnet', and each must be named if used.\n",
       "\n",
       "\\item[\\code{fit}] For \\code{relax.glmnet} a fitted 'glmnet' object\n",
       "\n",
       "\\item[\\code{maxp}] a limit on how many relaxed coefficients are allowed. Default is\n",
       "'n-3', where 'n' is the sample size. This may not be sufficient for\n",
       "non-gaussian familes, in which case users should supply a smaller value.\n",
       "This argument can be supplied directly to 'glmnet'.\n",
       "\n",
       "\\item[\\code{path}] Since \\code{glmnet} does not do stepsize optimization, the Newton\n",
       "algorithm can get stuck and not converge, especially with relaxed fits. With \\code{path=TRUE},\n",
       "each relaxed fit on a particular set of variables is computed pathwise using the original sequence\n",
       "of lambda values (with a zero attached to the end). Not needed for Gaussian models, and should not\n",
       "be used unless needed, since will lead to longer compute times. Default is \\code{path=FALSE}.\n",
       "appropriate subset of variables\n",
       "\n",
       "\\item[\\code{check.args}] Should \\code{relax.glmnet} make sure that all the data\n",
       "dependent arguments used in creating 'fit' have been resupplied. Default is\n",
       "'TRUE'.\n",
       "\\end{ldescription}\n",
       "\\end{Arguments}\n",
       "%\n",
       "\\begin{Details}\\relax\n",
       "The sequence of models implied by \\code{lambda} is fit by coordinate\n",
       "descent. For \\code{family=\"gaussian\"} this is the lasso sequence if\n",
       "\\code{alpha=1}, else it is the elasticnet sequence.\n",
       "\n",
       "The objective function for \\code{\"gaussian\"} is \\deqn{1/2 RSS/nobs +\n",
       "\\lambda*penalty,}{} and for the other models it is \\deqn{-loglik/nobs +\n",
       "\\lambda*penalty.}{} Note also that for \\code{\"gaussian\"}, \\code{glmnet}\n",
       "standardizes y to have unit variance (using 1/n rather than 1/(n-1) formula)\n",
       "before computing its lambda sequence (and then unstandardizes the resulting\n",
       "coefficients); if you wish to reproduce/compare results with other software,\n",
       "best to supply a standardized y. The coefficients for any predictor\n",
       "variables with zero variance are set to zero for all values of lambda.\n",
       "%\n",
       "\\begin{SubSection}{Details on \\code{family} option}\n",
       "\n",
       "From version 4.0 onwards, glmnet supports both the original built-in families,\n",
       "as well as \\emph{any} family object as used by \\code{stats:glm()}.\n",
       "This opens the door to a wide variety of additional models. For example\n",
       "\\code{family=binomial(link=cloglog)} or \\code{family=negative.binomial(theta=1.5)} (from the MASS library).\n",
       "Note that the code runs faster for the built-in families.\n",
       "\n",
       "The built in families are specifed via a character string. For all families,\n",
       "the object produced is a lasso or elasticnet regularization path for fitting the\n",
       "generalized linear regression paths, by maximizing the appropriate penalized\n",
       "log-likelihood (partial likelihood for the \"cox\" model). Sometimes the\n",
       "sequence is truncated before \\code{nlambda} values of \\code{lambda} have\n",
       "been used, because of instabilities in the inverse link functions near a\n",
       "saturated fit. \\code{glmnet(...,family=\"binomial\")} fits a traditional\n",
       "logistic regression model for the log-odds.\n",
       "\\code{glmnet(...,family=\"multinomial\")} fits a symmetric multinomial model,\n",
       "where each class is represented by a linear model (on the log-scale). The\n",
       "penalties take care of redundancies. A two-class \\code{\"multinomial\"} model\n",
       "will produce the same fit as the corresponding \\code{\"binomial\"} model,\n",
       "except the pair of coefficient matrices will be equal in magnitude and\n",
       "opposite in sign, and half the \\code{\"binomial\"} values.\n",
       "Two useful additional families are the \\code{family=\"mgaussian\"} family and\n",
       "the \\code{type.multinomial=\"grouped\"} option for multinomial fitting. The\n",
       "former allows a multi-response gaussian model to be fit, using a \"group\n",
       "-lasso\" penalty on the coefficients for each variable. Tying the responses\n",
       "together like this is called \"multi-task\" learning in some domains. The\n",
       "grouped multinomial allows the same penalty for the\n",
       "\\code{family=\"multinomial\"} model, which is also multi-responsed. For both\n",
       "of these the penalty on the coefficient vector for variable j is\n",
       "\\deqn{(1-\\alpha)/2||\\beta_j||_2^2+\\alpha||\\beta_j||_2.}{} When \\code{alpha=1}\n",
       "this is a group-lasso penalty, and otherwise it mixes with quadratic just\n",
       "like elasticnet. A small detail in the Cox model: if death times are tied\n",
       "with censored times, we assume the censored times occurred just\n",
       "\\emph{before} the death times in computing the Breslow approximation; if\n",
       "users prefer the usual convention of \\emph{after}, they can add a small\n",
       "number to all censoring times to achieve this effect.\n",
       "\\end{SubSection}\n",
       "\n",
       "\n",
       "%\n",
       "\\begin{SubSection}{Details on response for \\code{family=\"cox\"}}\n",
       "\n",
       "For Cox models, the response should preferably be a \\code{Surv} object,\n",
       "created by the \\code{Surv()} function in \\pkg{survival} package. For\n",
       "right-censored data, this object should have type \"right\", and for\n",
       "(start, stop] data, it should have type \"counting\". To fit stratified Cox\n",
       "models, strata should be added to the response via the \\code{stratifySurv()}\n",
       "function before passing the response to \\code{glmnet()}. (For backward\n",
       "compatibility, right-censored data can also be passed as a\n",
       "two-column matrix with columns named 'time' and 'status'. The\n",
       "latter is a binary variable, with '1' indicating death, and '0' indicating\n",
       "right censored.)\n",
       "\\end{SubSection}\n",
       "\n",
       "\n",
       "%\n",
       "\\begin{SubSection}{Details on \\code{relax} option}\n",
       "\n",
       "If \\code{relax=TRUE}\n",
       "a duplicate sequence of models is produced, where each active set in the\n",
       "elastic-net path is refit without regularization. The result of this is a\n",
       "matching \\code{\"glmnet\"} object which is stored on the original object in a\n",
       "component named \\code{\"relaxed\"}, and is part of the glmnet output.\n",
       "Generally users will not call \\code{relax.glmnet} directly, unless the\n",
       "original 'glmnet' object took a long time to fit. But if they do, they must\n",
       "supply the fit, and all the original arguments used to create that fit. They\n",
       "can limit the length of the relaxed path via 'maxp'.\n",
       "\\end{SubSection}\n",
       "\n",
       "\\end{Details}\n",
       "%\n",
       "\\begin{Value}\n",
       "An object with S3 class \\code{\"glmnet\",\"*\" }, where \\code{\"*\"} is\n",
       "\\code{\"elnet\"}, \\code{\"lognet\"}, \\code{\"multnet\"}, \\code{\"fishnet\"}\n",
       "(poisson), \\code{\"coxnet\"} or \\code{\"mrelnet\"} for the various types of\n",
       "models. If the model was created with \\code{relax=TRUE} then this class has\n",
       "a prefix class of \\code{\"relaxed\"}.  \\begin{ldescription}\n",
       "\\item[\\code{call}] the call that produced this\n",
       "object\\item[\\code{a0}] Intercept sequence of length \\code{length(lambda)}\n",
       "\\item[\\code{beta}] For \\code{\"elnet\"}, \\code{\"lognet\"}, \\code{\"fishnet\"} and\n",
       "\\code{\"coxnet\"} models, a \\code{nvars x length(lambda)} matrix of\n",
       "coefficients, stored in sparse column format (\\code{\"CsparseMatrix\"}). For\n",
       "\\code{\"multnet\"} and \\code{\"mgaussian\"}, a list of \\code{nc} such matrices,\n",
       "one for each class.\\item[\\code{lambda}] The actual sequence of \\code{lambda}\n",
       "values used. When \\code{alpha=0}, the largest lambda reported does not quite\n",
       "give the zero coefficients reported (\\code{lambda=inf} would in principle).\n",
       "Instead, the largest \\code{lambda} for \\code{alpha=0.001} is used, and the\n",
       "sequence of \\code{lambda} values is derived from this.\\item[\\code{dev.ratio}] The\n",
       "fraction of (null) deviance explained (for \\code{\"elnet\"}, this is the\n",
       "R-square). The deviance calculations incorporate weights if present in the\n",
       "model. The deviance is defined to be 2*(loglike\\_sat - loglike), where\n",
       "loglike\\_sat is the log-likelihood for the saturated model (a model with a\n",
       "free parameter per observation). Hence dev.ratio=1-dev/nulldev.\n",
       "\\item[\\code{nulldev}] Null deviance (per observation). This is defined to be\n",
       "2*(loglike\\_sat -loglike(Null)); The NULL model refers to the intercept\n",
       "model, except for the Cox, where it is the 0 model.\\item[\\code{df}] The number of\n",
       "nonzero coefficients for each value of \\code{lambda}. For \\code{\"multnet\"},\n",
       "this is the number of variables with a nonzero coefficient for \\emph{any}\n",
       "class.\\item[\\code{dfmat}] For \\code{\"multnet\"} and \\code{\"mrelnet\"} only. A\n",
       "matrix consisting of the number of nonzero coefficients per class\n",
       "\\item[\\code{dim}] dimension of coefficient matrix (ices)\\item[\\code{nobs}] number of\n",
       "observations\\item[\\code{npasses}] total passes over the data summed over all\n",
       "lambda values\\item[\\code{offset}] a logical variable indicating whether an offset\n",
       "was included in the model\\item[\\code{jerr}] error flag, for warnings and errors\n",
       "(largely for internal debugging).\\item[\\code{relaxed}] If \\code{relax=TRUE}, this\n",
       "additional item is another glmnet object with different values for\n",
       "\\code{beta} and \\code{dev.ratio}\n",
       "\\end{ldescription}\n",
       "\\end{Value}\n",
       "%\n",
       "\\begin{Author}\\relax\n",
       "Jerome Friedman, Trevor Hastie, Balasubramanian Narasimhan, Noah\n",
       "Simon, Kenneth Tay and Rob Tibshirani\\\\{} Maintainer: Trevor Hastie\n",
       "\\email{hastie@stanford.edu}\n",
       "\\end{Author}\n",
       "%\n",
       "\\begin{References}\\relax\n",
       "Friedman, J., Hastie, T. and Tibshirani, R. (2008)\n",
       "\\emph{Regularization Paths for Generalized Linear Models via Coordinate\n",
       "Descent (2010), Journal of Statistical Software, Vol. 33(1), 1-22},\n",
       "\\url{https://web.stanford.edu/~hastie/Papers/glmnet.pdf}.\\\\{}\n",
       "Simon, N., Friedman, J., Hastie, T. and Tibshirani, R. (2011)\n",
       "\\emph{Regularization Paths for Cox's Proportional\n",
       "Hazards Model via Coordinate Descent, Journal of Statistical Software, Vol.\n",
       "39(5), 1-13}, \\url{https://www.jstatsoft.org/v39/i05/}.\\\\{} Tibshirani,\n",
       "Robert, Bien, J., Friedman, J., Hastie, T.,Simon, N.,Taylor, J. and\n",
       "Tibshirani, Ryan. (2012) \\emph{Strong Rules for Discarding Predictors in\n",
       "Lasso-type Problems, JRSSB, Vol. 74(2), 245-266},\n",
       "\\url{https://statweb.stanford.edu/~tibs/ftp/strong.pdf}.\\\\{}\n",
       "Hastie, T., Tibshirani, Robert and Tibshirani, Ryan. \\emph{Extended\n",
       "Comparisons of Best Subset Selection, Forward Stepwise Selection, and the\n",
       "Lasso (2017), Stanford Statistics Technical Report},\n",
       "\\url{https://arxiv.org/abs/1707.08692}.\\\\{}\n",
       "Glmnet webpage with four vignettes, \\url{https://glmnet.stanford.edu}.\n",
       "\\end{References}\n",
       "%\n",
       "\\begin{SeeAlso}\\relax\n",
       "\\code{print}, \\code{predict}, \\code{coef} and \\code{plot} methods,\n",
       "and the \\code{cv.glmnet} function.\n",
       "\\end{SeeAlso}\n",
       "%\n",
       "\\begin{Examples}\n",
       "\\begin{ExampleCode}\n",
       "\n",
       "# Gaussian\n",
       "x = matrix(rnorm(100 * 20), 100, 20)\n",
       "y = rnorm(100)\n",
       "fit1 = glmnet(x, y)\n",
       "print(fit1)\n",
       "coef(fit1, s = 0.01)  # extract coefficients at a single value of lambda\n",
       "predict(fit1, newx = x[1:10, ], s = c(0.01, 0.005))  # make predictions\n",
       "\n",
       "# Relaxed\n",
       "fit1r = glmnet(x, y, relax = TRUE)  # can be used with any model\n",
       "\n",
       "# multivariate gaussian\n",
       "y = matrix(rnorm(100 * 3), 100, 3)\n",
       "fit1m = glmnet(x, y, family = \"mgaussian\")\n",
       "plot(fit1m, type.coef = \"2norm\")\n",
       "\n",
       "# binomial\n",
       "g2 = sample(c(0,1), 100, replace = TRUE)\n",
       "fit2 = glmnet(x, g2, family = \"binomial\")\n",
       "fit2n = glmnet(x, g2, family = binomial(link=cloglog))\n",
       "fit2r = glmnet(x,g2, family = \"binomial\", relax=TRUE)\n",
       "fit2rp = glmnet(x,g2, family = \"binomial\", relax=TRUE, path=TRUE)\n",
       "\n",
       "# multinomial\n",
       "g4 = sample(1:4, 100, replace = TRUE)\n",
       "fit3 = glmnet(x, g4, family = \"multinomial\")\n",
       "fit3a = glmnet(x, g4, family = \"multinomial\", type.multinomial = \"grouped\")\n",
       "# poisson\n",
       "N = 500\n",
       "p = 20\n",
       "nzc = 5\n",
       "x = matrix(rnorm(N * p), N, p)\n",
       "beta = rnorm(nzc)\n",
       "f = x[, seq(nzc)] %*% beta\n",
       "mu = exp(f)\n",
       "y = rpois(N, mu)\n",
       "fit = glmnet(x, y, family = \"poisson\")\n",
       "plot(fit)\n",
       "pfit = predict(fit, x, s = 0.001, type = \"response\")\n",
       "plot(pfit, y)\n",
       "\n",
       "# Cox\n",
       "set.seed(10101)\n",
       "N = 1000\n",
       "p = 30\n",
       "nzc = p/3\n",
       "x = matrix(rnorm(N * p), N, p)\n",
       "beta = rnorm(nzc)\n",
       "fx = x[, seq(nzc)] %*% beta/3\n",
       "hx = exp(fx)\n",
       "ty = rexp(N, hx)\n",
       "tcens = rbinom(n = N, prob = 0.3, size = 1)  # censoring indicator\n",
       "y = cbind(time = ty, status = 1 - tcens)  # y=Surv(ty,1-tcens) with library(survival)\n",
       "fit = glmnet(x, y, family = \"cox\")\n",
       "plot(fit)\n",
       "\n",
       "# Cox example with (start, stop] data\n",
       "set.seed(2)\n",
       "nobs <- 100; nvars <- 15\n",
       "xvec <- rnorm(nobs * nvars)\n",
       "xvec[sample.int(nobs * nvars, size = 0.4 * nobs * nvars)] <- 0\n",
       "x <- matrix(xvec, nrow = nobs)\n",
       "start_time <- runif(100, min = 0, max = 5)\n",
       "stop_time <- start_time + runif(100, min = 0.1, max = 3)\n",
       "status <- rbinom(n = nobs, prob = 0.3, size = 1)\n",
       "jsurv_ss <- survival::Surv(start_time, stop_time, status)\n",
       "fit <- glmnet(x, jsurv_ss, family = \"cox\")\n",
       "\n",
       "# Cox example with strata\n",
       "jsurv_ss2 <- stratifySurv(jsurv_ss, rep(1:2, each = 50))\n",
       "fit <- glmnet(x, jsurv_ss2, family = \"cox\")\n",
       "\n",
       "# Sparse\n",
       "n = 10000\n",
       "p = 200\n",
       "nzc = trunc(p/10)\n",
       "x = matrix(rnorm(n * p), n, p)\n",
       "iz = sample(1:(n * p), size = n * p * 0.85, replace = FALSE)\n",
       "x[iz] = 0\n",
       "sx = Matrix(x, sparse = TRUE)\n",
       "inherits(sx, \"sparseMatrix\")  #confirm that it is sparse\n",
       "beta = rnorm(nzc)\n",
       "fx = x[, seq(nzc)] %*% beta\n",
       "eps = rnorm(n)\n",
       "y = fx + eps\n",
       "px = exp(fx)\n",
       "px = px/(1 + px)\n",
       "ly = rbinom(n = length(px), prob = px, size = 1)\n",
       "system.time(fit1 <- glmnet(sx, y))\n",
       "system.time(fit2n <- glmnet(x, y))\n",
       "\n",
       "\\end{ExampleCode}\n",
       "\\end{Examples}"
      ],
      "text/plain": [
       "glmnet                 package:glmnet                  R Documentation\n",
       "\n",
       "_\bf_\bi_\bt _\ba _\bG_\bL_\bM _\bw_\bi_\bt_\bh _\bl_\ba_\bs_\bs_\bo _\bo_\br _\be_\bl_\ba_\bs_\bt_\bi_\bc_\bn_\be_\bt _\br_\be_\bg_\bu_\bl_\ba_\br_\bi_\bz_\ba_\bt_\bi_\bo_\bn\n",
       "\n",
       "_\bD_\be_\bs_\bc_\br_\bi_\bp_\bt_\bi_\bo_\bn:\n",
       "\n",
       "     Fit a generalized linear model via penalized maximum likelihood.\n",
       "     The regularization path is computed for the lasso or elasticnet\n",
       "     penalty at a grid of values for the regularization parameter\n",
       "     lambda. Can deal with all shapes of data, including very large\n",
       "     sparse data matrices. Fits linear, logistic and multinomial,\n",
       "     poisson, and Cox regression models.\n",
       "\n",
       "_\bU_\bs_\ba_\bg_\be:\n",
       "\n",
       "     glmnet(\n",
       "       x,\n",
       "       y,\n",
       "       family = c(\"gaussian\", \"binomial\", \"poisson\", \"multinomial\", \"cox\", \"mgaussian\"),\n",
       "       weights = NULL,\n",
       "       offset = NULL,\n",
       "       alpha = 1,\n",
       "       nlambda = 100,\n",
       "       lambda.min.ratio = ifelse(nobs < nvars, 0.01, 1e-04),\n",
       "       lambda = NULL,\n",
       "       standardize = TRUE,\n",
       "       intercept = TRUE,\n",
       "       thresh = 1e-07,\n",
       "       dfmax = nvars + 1,\n",
       "       pmax = min(dfmax * 2 + 20, nvars),\n",
       "       exclude = NULL,\n",
       "       penalty.factor = rep(1, nvars),\n",
       "       lower.limits = -Inf,\n",
       "       upper.limits = Inf,\n",
       "       maxit = 1e+05,\n",
       "       type.gaussian = ifelse(nvars < 500, \"covariance\", \"naive\"),\n",
       "       type.logistic = c(\"Newton\", \"modified.Newton\"),\n",
       "       standardize.response = FALSE,\n",
       "       type.multinomial = c(\"ungrouped\", \"grouped\"),\n",
       "       relax = FALSE,\n",
       "       trace.it = 0,\n",
       "       ...\n",
       "     )\n",
       "     \n",
       "     relax.glmnet(fit, x, ..., maxp = n - 3, path = FALSE, check.args = TRUE)\n",
       "     \n",
       "_\bA_\br_\bg_\bu_\bm_\be_\bn_\bt_\bs:\n",
       "\n",
       "       x: input matrix, of dimension nobs x nvars; each row is an\n",
       "          observation vector. Can be in sparse matrix format (inherit\n",
       "          from class '\"sparseMatrix\"' as in package 'Matrix')\n",
       "\n",
       "       y: response variable. Quantitative for 'family=\"gaussian\"', or\n",
       "          'family=\"poisson\"' (non-negative counts). For\n",
       "          'family=\"binomial\"' should be either a factor with two\n",
       "          levels, or a two-column matrix of counts or proportions (the\n",
       "          second column is treated as the target class; for a factor,\n",
       "          the last level in alphabetical order is the target class).\n",
       "          For 'family=\"multinomial\"', can be a 'nc>=2' level factor, or\n",
       "          a matrix with 'nc' columns of counts or proportions. For\n",
       "          either '\"binomial\"' or '\"multinomial\"', if 'y' is presented\n",
       "          as a vector, it will be coerced into a factor. For\n",
       "          'family=\"cox\"', preferably a 'Surv' object from the survival\n",
       "          package: see Details section for more information. For\n",
       "          'family=\"mgaussian\"', 'y' is a matrix of quantitative\n",
       "          responses.\n",
       "\n",
       "  family: Either a character string representing one of the built-in\n",
       "          families, or else a 'glm()' family object. For more\n",
       "          information, see Details section below or the documentation\n",
       "          for response type (above).\n",
       "\n",
       " weights: observation weights. Can be total counts if responses are\n",
       "          proportion matrices. Default is 1 for each observation\n",
       "\n",
       "  offset: A vector of length 'nobs' that is included in the linear\n",
       "          predictor (a 'nobs x nc' matrix for the '\"multinomial\"'\n",
       "          family). Useful for the '\"poisson\"' family (e.g. log of\n",
       "          exposure time), or for refining a model by starting at a\n",
       "          current fit. Default is 'NULL'. If supplied, then values must\n",
       "          also be supplied to the 'predict' function.\n",
       "\n",
       "   alpha: The elasticnet mixing parameter, with 0<=alpha<= 1. The\n",
       "          penalty is defined as\n",
       "\n",
       "                     (1-alpha)/2||beta||_2^2+alpha||beta||_1.           \n",
       "          \n",
       "          'alpha=1' is the lasso penalty, and 'alpha=0' the ridge\n",
       "          penalty.\n",
       "\n",
       " nlambda: The number of 'lambda' values - default is 100.\n",
       "\n",
       "lambda.min.ratio: Smallest value for 'lambda', as a fraction of\n",
       "          'lambda.max', the (data derived) entry value (i.e. the\n",
       "          smallest value for which all coefficients are zero). The\n",
       "          default depends on the sample size 'nobs' relative to the\n",
       "          number of variables 'nvars'. If 'nobs > nvars', the default\n",
       "          is '0.0001', close to zero.  If 'nobs < nvars', the default\n",
       "          is '0.01'.  A very small value of 'lambda.min.ratio' will\n",
       "          lead to a saturated fit in the 'nobs < nvars' case. This is\n",
       "          undefined for '\"binomial\"' and '\"multinomial\"' models, and\n",
       "          'glmnet' will exit gracefully when the percentage deviance\n",
       "          explained is almost 1.\n",
       "\n",
       "  lambda: A user supplied 'lambda' sequence. Typical usage is to have\n",
       "          the program compute its own 'lambda' sequence based on\n",
       "          'nlambda' and 'lambda.min.ratio'. Supplying a value of\n",
       "          'lambda' overrides this. WARNING: use with care. Avoid\n",
       "          supplying a single value for 'lambda' (for predictions after\n",
       "          CV use 'predict()' instead).  Supply instead a decreasing\n",
       "          sequence of 'lambda' values. 'glmnet' relies on its warms\n",
       "          starts for speed, and its often faster to fit a whole path\n",
       "          than compute a single fit.\n",
       "\n",
       "standardize: Logical flag for x variable standardization, prior to\n",
       "          fitting the model sequence. The coefficients are always\n",
       "          returned on the original scale. Default is\n",
       "          'standardize=TRUE'.  If variables are in the same units\n",
       "          already, you might not wish to standardize. See details below\n",
       "          for y standardization with 'family=\"gaussian\"'.\n",
       "\n",
       "intercept: Should intercept(s) be fitted (default=TRUE) or set to zero\n",
       "          (FALSE)\n",
       "\n",
       "  thresh: Convergence threshold for coordinate descent. Each inner\n",
       "          coordinate-descent loop continues until the maximum change in\n",
       "          the objective after any coefficient update is less than\n",
       "          'thresh' times the null deviance. Defaults value is '1E-7'.\n",
       "\n",
       "   dfmax: Limit the maximum number of variables in the model. Useful\n",
       "          for very large 'nvars', if a partial path is desired.\n",
       "\n",
       "    pmax: Limit the maximum number of variables ever to be nonzero\n",
       "\n",
       " exclude: Indices of variables to be excluded from the model. Default\n",
       "          is none. Equivalent to an infinite penalty factor (next\n",
       "          item).\n",
       "\n",
       "penalty.factor: Separate penalty factors can be applied to each\n",
       "          coefficient. This is a number that multiplies 'lambda' to\n",
       "          allow differential shrinkage. Can be 0 for some variables,\n",
       "          which implies no shrinkage, and that variable is always\n",
       "          included in the model. Default is 1 for all variables (and\n",
       "          implicitly infinity for variables listed in 'exclude'). Note:\n",
       "          the penalty factors are internally rescaled to sum to nvars,\n",
       "          and the lambda sequence will reflect this change.\n",
       "\n",
       "lower.limits: Vector of lower limits for each coefficient; default\n",
       "          '-Inf'. Each of these must be non-positive. Can be presented\n",
       "          as a single value (which will then be replicated), else a\n",
       "          vector of length 'nvars'\n",
       "\n",
       "upper.limits: Vector of upper limits for each coefficient; default\n",
       "          'Inf'. See 'lower.limits'\n",
       "\n",
       "   maxit: Maximum number of passes over the data for all lambda values;\n",
       "          default is 10^5.\n",
       "\n",
       "type.gaussian: Two algorithm types are supported for (only)\n",
       "          'family=\"gaussian\"'. The default when 'nvar<500' is\n",
       "          'type.gaussian=\"covariance\"', and saves all inner-products\n",
       "          ever computed. This can be much faster than\n",
       "          'type.gaussian=\"naive\"', which loops through 'nobs' every\n",
       "          time an inner-product is computed. The latter can be far more\n",
       "          efficient for 'nvar >> nobs' situations, or when 'nvar >\n",
       "          500'.\n",
       "\n",
       "type.logistic: If '\"Newton\"' then the exact hessian is used (default),\n",
       "          while '\"modified.Newton\"' uses an upper-bound on the hessian,\n",
       "          and can be faster.\n",
       "\n",
       "standardize.response: This is for the 'family=\"mgaussian\"' family, and\n",
       "          allows the user to standardize the response variables\n",
       "\n",
       "type.multinomial: If '\"grouped\"' then a grouped lasso penalty is used\n",
       "          on the multinomial coefficients for a variable. This ensures\n",
       "          they are all in our out together. The default is\n",
       "          '\"ungrouped\"'\n",
       "\n",
       "   relax: If 'TRUE' then for each _active set_ in the path of\n",
       "          solutions, the model is refit without any regularization. See\n",
       "          'details' for more information. This argument is new, and\n",
       "          users may experience convergence issues with small datasets,\n",
       "          especially with non-gaussian families. Limiting the value of\n",
       "          'maxp' can alleviate these issues in some cases.\n",
       "\n",
       "trace.it: If 'trace.it=1', then a progress bar is displayed; useful for\n",
       "          big models that take a long time to fit.\n",
       "\n",
       "     ...: Additional argument used in 'relax.glmnet'. These include\n",
       "          some of the original arguments to 'glmnet', and each must be\n",
       "          named if used.\n",
       "\n",
       "     fit: For 'relax.glmnet' a fitted 'glmnet' object\n",
       "\n",
       "    maxp: a limit on how many relaxed coefficients are allowed. Default\n",
       "          is 'n-3', where 'n' is the sample size. This may not be\n",
       "          sufficient for non-gaussian familes, in which case users\n",
       "          should supply a smaller value. This argument can be supplied\n",
       "          directly to 'glmnet'.\n",
       "\n",
       "    path: Since 'glmnet' does not do stepsize optimization, the Newton\n",
       "          algorithm can get stuck and not converge, especially with\n",
       "          relaxed fits. With 'path=TRUE', each relaxed fit on a\n",
       "          particular set of variables is computed pathwise using the\n",
       "          original sequence of lambda values (with a zero attached to\n",
       "          the end). Not needed for Gaussian models, and should not be\n",
       "          used unless needed, since will lead to longer compute times.\n",
       "          Default is 'path=FALSE'. appropriate subset of variables\n",
       "\n",
       "check.args: Should 'relax.glmnet' make sure that all the data dependent\n",
       "          arguments used in creating 'fit' have been resupplied.\n",
       "          Default is 'TRUE'.\n",
       "\n",
       "_\bD_\be_\bt_\ba_\bi_\bl_\bs:\n",
       "\n",
       "     The sequence of models implied by 'lambda' is fit by coordinate\n",
       "     descent. For 'family=\"gaussian\"' this is the lasso sequence if\n",
       "     'alpha=1', else it is the elasticnet sequence.\n",
       "\n",
       "     The objective function for '\"gaussian\"' is\n",
       "\n",
       "                        1/2 RSS/nobs +lambda*penalty,                   \n",
       "     \n",
       "     and for the other models it is\n",
       "\n",
       "                        -loglik/nobs +lambda*penalty.                   \n",
       "     \n",
       "     Note also that for '\"gaussian\"', 'glmnet' standardizes y to have\n",
       "     unit variance (using 1/n rather than 1/(n-1) formula) before\n",
       "     computing its lambda sequence (and then unstandardizes the\n",
       "     resulting coefficients); if you wish to reproduce/compare results\n",
       "     with other software, best to supply a standardized y. The\n",
       "     coefficients for any predictor variables with zero variance are\n",
       "     set to zero for all values of lambda.\n",
       "\n",
       "  _\bD_\be_\bt_\ba_\bi_\bl_\bs _\bo_\bn '_\bf_\ba_\bm_\bi_\bl_\by' _\bo_\bp_\bt_\bi_\bo_\bn:\n",
       "\n",
       "       From version 4.0 onwards, glmnet supports both the original\n",
       "       built-in families, as well as _any_ family object as used by\n",
       "       'stats:glm()'. This opens the door to a wide variety of\n",
       "       additional models. For example 'family=binomial(link=cloglog)'\n",
       "       or 'family=negative.binomial(theta=1.5)' (from the MASS\n",
       "       library). Note that the code runs faster for the built-in\n",
       "       families.\n",
       "\n",
       "       The built in families are specifed via a character string. For\n",
       "       all families, the object produced is a lasso or elasticnet\n",
       "       regularization path for fitting the generalized linear\n",
       "       regression paths, by maximizing the appropriate penalized\n",
       "       log-likelihood (partial likelihood for the \"cox\" model).\n",
       "       Sometimes the sequence is truncated before 'nlambda' values of\n",
       "       'lambda' have been used, because of instabilities in the inverse\n",
       "       link functions near a saturated fit.\n",
       "       'glmnet(...,family=\"binomial\")' fits a traditional logistic\n",
       "       regression model for the log-odds.\n",
       "       'glmnet(...,family=\"multinomial\")' fits a symmetric multinomial\n",
       "       model, where each class is represented by a linear model (on the\n",
       "       log-scale). The penalties take care of redundancies. A two-class\n",
       "       '\"multinomial\"' model will produce the same fit as the\n",
       "       corresponding '\"binomial\"' model, except the pair of coefficient\n",
       "       matrices will be equal in magnitude and opposite in sign, and\n",
       "       half the '\"binomial\"' values. Two useful additional families are\n",
       "       the 'family=\"mgaussian\"' family and the\n",
       "       'type.multinomial=\"grouped\"' option for multinomial fitting. The\n",
       "       former allows a multi-response gaussian model to be fit, using a\n",
       "       \"group -lasso\" penalty on the coefficients for each variable.\n",
       "       Tying the responses together like this is called \"multi-task\"\n",
       "       learning in some domains. The grouped multinomial allows the\n",
       "       same penalty for the 'family=\"multinomial\"' model, which is also\n",
       "       multi-responsed. For both of these the penalty on the\n",
       "       coefficient vector for variable j is\n",
       "\n",
       "                 (1-alpha)/2||beta_j||_2^2+alpha||beta_j||_2.           \n",
       "       \n",
       "       When 'alpha=1' this is a group-lasso penalty, and otherwise it\n",
       "       mixes with quadratic just like elasticnet. A small detail in the\n",
       "       Cox model: if death times are tied with censored times, we\n",
       "       assume the censored times occurred just _before_ the death times\n",
       "       in computing the Breslow approximation; if users prefer the\n",
       "       usual convention of _after_, they can add a small number to all\n",
       "       censoring times to achieve this effect.\n",
       "\n",
       "\n",
       "  _\bD_\be_\bt_\ba_\bi_\bl_\bs _\bo_\bn _\br_\be_\bs_\bp_\bo_\bn_\bs_\be _\bf_\bo_\br '_\bf_\ba_\bm_\bi_\bl_\by=\"_\bc_\bo_\bx\"':\n",
       "\n",
       "       For Cox models, the response should preferably be a 'Surv'\n",
       "       object, created by the 'Surv()' function in 'survival' package.\n",
       "       For right-censored data, this object should have type \"right\",\n",
       "       and for (start, stop] data, it should have type \"counting\". To\n",
       "       fit stratified Cox models, strata should be added to the\n",
       "       response via the 'stratifySurv()' function before passing the\n",
       "       response to 'glmnet()'. (For backward compatibility,\n",
       "       right-censored data can also be passed as a two-column matrix\n",
       "       with columns named 'time' and 'status'. The latter is a binary\n",
       "       variable, with '1' indicating death, and '0' indicating right\n",
       "       censored.)\n",
       "\n",
       "\n",
       "  _\bD_\be_\bt_\ba_\bi_\bl_\bs _\bo_\bn '_\br_\be_\bl_\ba_\bx' _\bo_\bp_\bt_\bi_\bo_\bn:\n",
       "\n",
       "       If 'relax=TRUE' a duplicate sequence of models is produced,\n",
       "       where each active set in the elastic-net path is refit without\n",
       "       regularization. The result of this is a matching '\"glmnet\"'\n",
       "       object which is stored on the original object in a component\n",
       "       named '\"relaxed\"', and is part of the glmnet output. Generally\n",
       "       users will not call 'relax.glmnet' directly, unless the original\n",
       "       'glmnet' object took a long time to fit. But if they do, they\n",
       "       must supply the fit, and all the original arguments used to\n",
       "       create that fit. They can limit the length of the relaxed path\n",
       "       via 'maxp'.\n",
       "\n",
       "\n",
       "_\bV_\ba_\bl_\bu_\be:\n",
       "\n",
       "     An object with S3 class '\"glmnet\",\"*\" ', where '\"*\"' is '\"elnet\"',\n",
       "     '\"lognet\"', '\"multnet\"', '\"fishnet\"' (poisson), '\"coxnet\"' or\n",
       "     '\"mrelnet\"' for the various types of models. If the model was\n",
       "     created with 'relax=TRUE' then this class has a prefix class of\n",
       "     '\"relaxed\"'.\n",
       "\n",
       "    call: the call that produced this object\n",
       "\n",
       "      a0: Intercept sequence of length 'length(lambda)'\n",
       "\n",
       "    beta: For '\"elnet\"', '\"lognet\"', '\"fishnet\"' and '\"coxnet\"' models,\n",
       "          a 'nvars x length(lambda)' matrix of coefficients, stored in\n",
       "          sparse column format ('\"CsparseMatrix\"'). For '\"multnet\"' and\n",
       "          '\"mgaussian\"', a list of 'nc' such matrices, one for each\n",
       "          class.\n",
       "\n",
       "  lambda: The actual sequence of 'lambda' values used. When 'alpha=0',\n",
       "          the largest lambda reported does not quite give the zero\n",
       "          coefficients reported ('lambda=inf' would in principle).\n",
       "          Instead, the largest 'lambda' for 'alpha=0.001' is used, and\n",
       "          the sequence of 'lambda' values is derived from this.\n",
       "\n",
       "dev.ratio: The fraction of (null) deviance explained (for '\"elnet\"',\n",
       "          this is the R-square). The deviance calculations incorporate\n",
       "          weights if present in the model. The deviance is defined to\n",
       "          be 2*(loglike_sat - loglike), where loglike_sat is the\n",
       "          log-likelihood for the saturated model (a model with a free\n",
       "          parameter per observation). Hence dev.ratio=1-dev/nulldev.\n",
       "\n",
       " nulldev: Null deviance (per observation). This is defined to be\n",
       "          2*(loglike_sat -loglike(Null)); The NULL model refers to the\n",
       "          intercept model, except for the Cox, where it is the 0 model.\n",
       "\n",
       "      df: The number of nonzero coefficients for each value of\n",
       "          'lambda'. For '\"multnet\"', this is the number of variables\n",
       "          with a nonzero coefficient for _any_ class.\n",
       "\n",
       "   dfmat: For '\"multnet\"' and '\"mrelnet\"' only. A matrix consisting of\n",
       "          the number of nonzero coefficients per class\n",
       "\n",
       "     dim: dimension of coefficient matrix (ices)\n",
       "\n",
       "    nobs: number of observations\n",
       "\n",
       " npasses: total passes over the data summed over all lambda values\n",
       "\n",
       "  offset: a logical variable indicating whether an offset was included\n",
       "          in the model\n",
       "\n",
       "    jerr: error flag, for warnings and errors (largely for internal\n",
       "          debugging).\n",
       "\n",
       " relaxed: If 'relax=TRUE', this additional item is another glmnet\n",
       "          object with different values for 'beta' and 'dev.ratio'\n",
       "\n",
       "_\bA_\bu_\bt_\bh_\bo_\br(_\bs):\n",
       "\n",
       "     Jerome Friedman, Trevor Hastie, Balasubramanian Narasimhan, Noah\n",
       "     Simon, Kenneth Tay and Rob Tibshirani\n",
       "     Maintainer: Trevor Hastie <email: hastie@stanford.edu>\n",
       "\n",
       "_\bR_\be_\bf_\be_\br_\be_\bn_\bc_\be_\bs:\n",
       "\n",
       "     Friedman, J., Hastie, T. and Tibshirani, R. (2008) _Regularization\n",
       "     Paths for Generalized Linear Models via Coordinate Descent (2010),\n",
       "     Journal of Statistical Software, Vol. 33(1), 1-22_, <URL:\n",
       "     https://web.stanford.edu/~hastie/Papers/glmnet.pdf>.\n",
       "     Simon, N., Friedman, J., Hastie, T. and Tibshirani, R. (2011)\n",
       "     _Regularization Paths for Cox's Proportional Hazards Model via\n",
       "     Coordinate Descent, Journal of Statistical Software, Vol. 39(5),\n",
       "     1-13_, <URL: https://www.jstatsoft.org/v39/i05/>.\n",
       "     Tibshirani, Robert, Bien, J., Friedman, J., Hastie, T.,Simon,\n",
       "     N.,Taylor, J. and Tibshirani, Ryan. (2012) _Strong Rules for\n",
       "     Discarding Predictors in Lasso-type Problems, JRSSB, Vol. 74(2),\n",
       "     245-266_, <URL:\n",
       "     https://statweb.stanford.edu/~tibs/ftp/strong.pdf>.\n",
       "     Hastie, T., Tibshirani, Robert and Tibshirani, Ryan. _Extended\n",
       "     Comparisons of Best Subset Selection, Forward Stepwise Selection,\n",
       "     and the Lasso (2017), Stanford Statistics Technical Report_, <URL:\n",
       "     https://arxiv.org/abs/1707.08692>.\n",
       "     Glmnet webpage with four vignettes, <URL:\n",
       "     https://glmnet.stanford.edu>.\n",
       "\n",
       "_\bS_\be_\be _\bA_\bl_\bs_\bo:\n",
       "\n",
       "     'print', 'predict', 'coef' and 'plot' methods, and the 'cv.glmnet'\n",
       "     function.\n",
       "\n",
       "_\bE_\bx_\ba_\bm_\bp_\bl_\be_\bs:\n",
       "\n",
       "     # Gaussian\n",
       "     x = matrix(rnorm(100 * 20), 100, 20)\n",
       "     y = rnorm(100)\n",
       "     fit1 = glmnet(x, y)\n",
       "     print(fit1)\n",
       "     coef(fit1, s = 0.01)  # extract coefficients at a single value of lambda\n",
       "     predict(fit1, newx = x[1:10, ], s = c(0.01, 0.005))  # make predictions\n",
       "     \n",
       "     # Relaxed\n",
       "     fit1r = glmnet(x, y, relax = TRUE)  # can be used with any model\n",
       "     \n",
       "     # multivariate gaussian\n",
       "     y = matrix(rnorm(100 * 3), 100, 3)\n",
       "     fit1m = glmnet(x, y, family = \"mgaussian\")\n",
       "     plot(fit1m, type.coef = \"2norm\")\n",
       "     \n",
       "     # binomial\n",
       "     g2 = sample(c(0,1), 100, replace = TRUE)\n",
       "     fit2 = glmnet(x, g2, family = \"binomial\")\n",
       "     fit2n = glmnet(x, g2, family = binomial(link=cloglog))\n",
       "     fit2r = glmnet(x,g2, family = \"binomial\", relax=TRUE)\n",
       "     fit2rp = glmnet(x,g2, family = \"binomial\", relax=TRUE, path=TRUE)\n",
       "     \n",
       "     # multinomial\n",
       "     g4 = sample(1:4, 100, replace = TRUE)\n",
       "     fit3 = glmnet(x, g4, family = \"multinomial\")\n",
       "     fit3a = glmnet(x, g4, family = \"multinomial\", type.multinomial = \"grouped\")\n",
       "     # poisson\n",
       "     N = 500\n",
       "     p = 20\n",
       "     nzc = 5\n",
       "     x = matrix(rnorm(N * p), N, p)\n",
       "     beta = rnorm(nzc)\n",
       "     f = x[, seq(nzc)] %*% beta\n",
       "     mu = exp(f)\n",
       "     y = rpois(N, mu)\n",
       "     fit = glmnet(x, y, family = \"poisson\")\n",
       "     plot(fit)\n",
       "     pfit = predict(fit, x, s = 0.001, type = \"response\")\n",
       "     plot(pfit, y)\n",
       "     \n",
       "     # Cox\n",
       "     set.seed(10101)\n",
       "     N = 1000\n",
       "     p = 30\n",
       "     nzc = p/3\n",
       "     x = matrix(rnorm(N * p), N, p)\n",
       "     beta = rnorm(nzc)\n",
       "     fx = x[, seq(nzc)] %*% beta/3\n",
       "     hx = exp(fx)\n",
       "     ty = rexp(N, hx)\n",
       "     tcens = rbinom(n = N, prob = 0.3, size = 1)  # censoring indicator\n",
       "     y = cbind(time = ty, status = 1 - tcens)  # y=Surv(ty,1-tcens) with library(survival)\n",
       "     fit = glmnet(x, y, family = \"cox\")\n",
       "     plot(fit)\n",
       "     \n",
       "     # Cox example with (start, stop] data\n",
       "     set.seed(2)\n",
       "     nobs <- 100; nvars <- 15\n",
       "     xvec <- rnorm(nobs * nvars)\n",
       "     xvec[sample.int(nobs * nvars, size = 0.4 * nobs * nvars)] <- 0\n",
       "     x <- matrix(xvec, nrow = nobs)\n",
       "     start_time <- runif(100, min = 0, max = 5)\n",
       "     stop_time <- start_time + runif(100, min = 0.1, max = 3)\n",
       "     status <- rbinom(n = nobs, prob = 0.3, size = 1)\n",
       "     jsurv_ss <- survival::Surv(start_time, stop_time, status)\n",
       "     fit <- glmnet(x, jsurv_ss, family = \"cox\")\n",
       "     \n",
       "     # Cox example with strata\n",
       "     jsurv_ss2 <- stratifySurv(jsurv_ss, rep(1:2, each = 50))\n",
       "     fit <- glmnet(x, jsurv_ss2, family = \"cox\")\n",
       "     \n",
       "     # Sparse\n",
       "     n = 10000\n",
       "     p = 200\n",
       "     nzc = trunc(p/10)\n",
       "     x = matrix(rnorm(n * p), n, p)\n",
       "     iz = sample(1:(n * p), size = n * p * 0.85, replace = FALSE)\n",
       "     x[iz] = 0\n",
       "     sx = Matrix(x, sparse = TRUE)\n",
       "     inherits(sx, \"sparseMatrix\")  #confirm that it is sparse\n",
       "     beta = rnorm(nzc)\n",
       "     fx = x[, seq(nzc)] %*% beta\n",
       "     eps = rnorm(n)\n",
       "     y = fx + eps\n",
       "     px = exp(fx)\n",
       "     px = px/(1 + px)\n",
       "     ly = rbinom(n = length(px), prob = px, size = 1)\n",
       "     system.time(fit1 <- glmnet(sx, y))\n",
       "     system.time(fit2n <- glmnet(x, y))\n",
       "     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?glmnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>1e-30</li><li>2.00923300256505e-30</li><li>4.03701725859655e-30</li><li>8.11130830789689e-30</li><li>1.62975083462064e-29</li><li>3.27454916287773e-29</li><li>6.57933224657571e-29</li><li>1.32194114846603e-28</li><li>2.65608778294669e-28</li><li>5.3366992312063e-28</li><li>1.07226722201033e-27</li><li>2.15443469003188e-27</li><li>4.32876128108306e-27</li><li>8.6974900261778e-27</li><li>1.74752840000768e-26</li><li>3.51119173421514e-26</li><li>7.05480231071863e-26</li><li>1.41747416292681e-25</li><li>2.84803586843579e-25</li><li>5.72236765935022e-25</li><li>1.14975699539774e-24</li><li>2.31012970008316e-24</li><li>4.64158883361279e-24</li><li>9.32603346883218e-24</li><li>1.87381742286039e-23</li><li>3.76493580679249e-23</li><li>7.56463327554629e-23</li><li>1.51991108295293e-22</li><li>3.05385550883341e-22</li><li>6.13590727341319e-22</li><li>1.23284673944207e-21</li><li>2.47707635599171e-21</li><li>4.97702356433209e-21</li><li>1e-20</li><li>2.00923300256505e-20</li><li>4.03701725859655e-20</li><li>8.11130830789689e-20</li><li>1.62975083462064e-19</li><li>3.27454916287773e-19</li><li>6.57933224657571e-19</li><li>1.32194114846603e-18</li><li>2.65608778294667e-18</li><li>5.3366992312063e-18</li><li>1.07226722201033e-17</li><li>2.1544346900319e-17</li><li>4.32876128108306e-17</li><li>8.6974900261778e-17</li><li>1.74752840000768e-16</li><li>3.51119173421514e-16</li><li>7.05480231071866e-16</li><li>1.41747416292681e-15</li><li>2.8480358684358e-15</li><li>5.72236765935022e-15</li><li>1.14975699539774e-14</li><li>2.31012970008316e-14</li><li>4.64158883361279e-14</li><li>9.32603346883218e-14</li><li>1.87381742286039e-13</li><li>3.76493580679249e-13</li><li>7.56463327554629e-13</li><li>1.51991108295294e-12</li><li>3.05385550883341e-12</li><li>6.13590727341319e-12</li><li>1.23284673944206e-11</li><li>2.47707635599171e-11</li><li>4.97702356433213e-11</li><li>1e-10</li><li>2.00923300256505e-10</li><li>4.03701725859655e-10</li><li>8.11130830789689e-10</li><li>1.62975083462064e-09</li><li>3.27454916287773e-09</li><li>6.57933224657571e-09</li><li>1.32194114846603e-08</li><li>2.65608778294669e-08</li><li>5.3366992312063e-08</li><li>1.07226722201033e-07</li><li>2.1544346900319e-07</li><li>4.32876128108306e-07</li><li>8.69749002617787e-07</li><li>1.74752840000768e-06</li><li>3.51119173421514e-06</li><li>7.05480231071863e-06</li><li>1.41747416292681e-05</li><li>2.84803586843582e-05</li><li>5.72236765935022e-05</li><li>0.000114975699539774</li><li>0.000231012970008316</li><li>0.000464158883361279</li><li>0.000932603346883218</li><li>0.00187381742286039</li><li>0.00376493580679249</li><li>0.00756463327554629</li><li>0.0151991108295294</li><li>0.0305385550883341</li><li>0.0613590727341319</li><li>0.123284673944207</li><li>0.247707635599171</li><li>0.497702356433213</li><li>1</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1e-30\n",
       "\\item 2.00923300256505e-30\n",
       "\\item 4.03701725859655e-30\n",
       "\\item 8.11130830789689e-30\n",
       "\\item 1.62975083462064e-29\n",
       "\\item 3.27454916287773e-29\n",
       "\\item 6.57933224657571e-29\n",
       "\\item 1.32194114846603e-28\n",
       "\\item 2.65608778294669e-28\n",
       "\\item 5.3366992312063e-28\n",
       "\\item 1.07226722201033e-27\n",
       "\\item 2.15443469003188e-27\n",
       "\\item 4.32876128108306e-27\n",
       "\\item 8.6974900261778e-27\n",
       "\\item 1.74752840000768e-26\n",
       "\\item 3.51119173421514e-26\n",
       "\\item 7.05480231071863e-26\n",
       "\\item 1.41747416292681e-25\n",
       "\\item 2.84803586843579e-25\n",
       "\\item 5.72236765935022e-25\n",
       "\\item 1.14975699539774e-24\n",
       "\\item 2.31012970008316e-24\n",
       "\\item 4.64158883361279e-24\n",
       "\\item 9.32603346883218e-24\n",
       "\\item 1.87381742286039e-23\n",
       "\\item 3.76493580679249e-23\n",
       "\\item 7.56463327554629e-23\n",
       "\\item 1.51991108295293e-22\n",
       "\\item 3.05385550883341e-22\n",
       "\\item 6.13590727341319e-22\n",
       "\\item 1.23284673944207e-21\n",
       "\\item 2.47707635599171e-21\n",
       "\\item 4.97702356433209e-21\n",
       "\\item 1e-20\n",
       "\\item 2.00923300256505e-20\n",
       "\\item 4.03701725859655e-20\n",
       "\\item 8.11130830789689e-20\n",
       "\\item 1.62975083462064e-19\n",
       "\\item 3.27454916287773e-19\n",
       "\\item 6.57933224657571e-19\n",
       "\\item 1.32194114846603e-18\n",
       "\\item 2.65608778294667e-18\n",
       "\\item 5.3366992312063e-18\n",
       "\\item 1.07226722201033e-17\n",
       "\\item 2.1544346900319e-17\n",
       "\\item 4.32876128108306e-17\n",
       "\\item 8.6974900261778e-17\n",
       "\\item 1.74752840000768e-16\n",
       "\\item 3.51119173421514e-16\n",
       "\\item 7.05480231071866e-16\n",
       "\\item 1.41747416292681e-15\n",
       "\\item 2.8480358684358e-15\n",
       "\\item 5.72236765935022e-15\n",
       "\\item 1.14975699539774e-14\n",
       "\\item 2.31012970008316e-14\n",
       "\\item 4.64158883361279e-14\n",
       "\\item 9.32603346883218e-14\n",
       "\\item 1.87381742286039e-13\n",
       "\\item 3.76493580679249e-13\n",
       "\\item 7.56463327554629e-13\n",
       "\\item 1.51991108295294e-12\n",
       "\\item 3.05385550883341e-12\n",
       "\\item 6.13590727341319e-12\n",
       "\\item 1.23284673944206e-11\n",
       "\\item 2.47707635599171e-11\n",
       "\\item 4.97702356433213e-11\n",
       "\\item 1e-10\n",
       "\\item 2.00923300256505e-10\n",
       "\\item 4.03701725859655e-10\n",
       "\\item 8.11130830789689e-10\n",
       "\\item 1.62975083462064e-09\n",
       "\\item 3.27454916287773e-09\n",
       "\\item 6.57933224657571e-09\n",
       "\\item 1.32194114846603e-08\n",
       "\\item 2.65608778294669e-08\n",
       "\\item 5.3366992312063e-08\n",
       "\\item 1.07226722201033e-07\n",
       "\\item 2.1544346900319e-07\n",
       "\\item 4.32876128108306e-07\n",
       "\\item 8.69749002617787e-07\n",
       "\\item 1.74752840000768e-06\n",
       "\\item 3.51119173421514e-06\n",
       "\\item 7.05480231071863e-06\n",
       "\\item 1.41747416292681e-05\n",
       "\\item 2.84803586843582e-05\n",
       "\\item 5.72236765935022e-05\n",
       "\\item 0.000114975699539774\n",
       "\\item 0.000231012970008316\n",
       "\\item 0.000464158883361279\n",
       "\\item 0.000932603346883218\n",
       "\\item 0.00187381742286039\n",
       "\\item 0.00376493580679249\n",
       "\\item 0.00756463327554629\n",
       "\\item 0.0151991108295294\n",
       "\\item 0.0305385550883341\n",
       "\\item 0.0613590727341319\n",
       "\\item 0.123284673944207\n",
       "\\item 0.247707635599171\n",
       "\\item 0.497702356433213\n",
       "\\item 1\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1e-30\n",
       "2. 2.00923300256505e-30\n",
       "3. 4.03701725859655e-30\n",
       "4. 8.11130830789689e-30\n",
       "5. 1.62975083462064e-29\n",
       "6. 3.27454916287773e-29\n",
       "7. 6.57933224657571e-29\n",
       "8. 1.32194114846603e-28\n",
       "9. 2.65608778294669e-28\n",
       "10. 5.3366992312063e-28\n",
       "11. 1.07226722201033e-27\n",
       "12. 2.15443469003188e-27\n",
       "13. 4.32876128108306e-27\n",
       "14. 8.6974900261778e-27\n",
       "15. 1.74752840000768e-26\n",
       "16. 3.51119173421514e-26\n",
       "17. 7.05480231071863e-26\n",
       "18. 1.41747416292681e-25\n",
       "19. 2.84803586843579e-25\n",
       "20. 5.72236765935022e-25\n",
       "21. 1.14975699539774e-24\n",
       "22. 2.31012970008316e-24\n",
       "23. 4.64158883361279e-24\n",
       "24. 9.32603346883218e-24\n",
       "25. 1.87381742286039e-23\n",
       "26. 3.76493580679249e-23\n",
       "27. 7.56463327554629e-23\n",
       "28. 1.51991108295293e-22\n",
       "29. 3.05385550883341e-22\n",
       "30. 6.13590727341319e-22\n",
       "31. 1.23284673944207e-21\n",
       "32. 2.47707635599171e-21\n",
       "33. 4.97702356433209e-21\n",
       "34. 1e-20\n",
       "35. 2.00923300256505e-20\n",
       "36. 4.03701725859655e-20\n",
       "37. 8.11130830789689e-20\n",
       "38. 1.62975083462064e-19\n",
       "39. 3.27454916287773e-19\n",
       "40. 6.57933224657571e-19\n",
       "41. 1.32194114846603e-18\n",
       "42. 2.65608778294667e-18\n",
       "43. 5.3366992312063e-18\n",
       "44. 1.07226722201033e-17\n",
       "45. 2.1544346900319e-17\n",
       "46. 4.32876128108306e-17\n",
       "47. 8.6974900261778e-17\n",
       "48. 1.74752840000768e-16\n",
       "49. 3.51119173421514e-16\n",
       "50. 7.05480231071866e-16\n",
       "51. 1.41747416292681e-15\n",
       "52. 2.8480358684358e-15\n",
       "53. 5.72236765935022e-15\n",
       "54. 1.14975699539774e-14\n",
       "55. 2.31012970008316e-14\n",
       "56. 4.64158883361279e-14\n",
       "57. 9.32603346883218e-14\n",
       "58. 1.87381742286039e-13\n",
       "59. 3.76493580679249e-13\n",
       "60. 7.56463327554629e-13\n",
       "61. 1.51991108295294e-12\n",
       "62. 3.05385550883341e-12\n",
       "63. 6.13590727341319e-12\n",
       "64. 1.23284673944206e-11\n",
       "65. 2.47707635599171e-11\n",
       "66. 4.97702356433213e-11\n",
       "67. 1e-10\n",
       "68. 2.00923300256505e-10\n",
       "69. 4.03701725859655e-10\n",
       "70. 8.11130830789689e-10\n",
       "71. 1.62975083462064e-09\n",
       "72. 3.27454916287773e-09\n",
       "73. 6.57933224657571e-09\n",
       "74. 1.32194114846603e-08\n",
       "75. 2.65608778294669e-08\n",
       "76. 5.3366992312063e-08\n",
       "77. 1.07226722201033e-07\n",
       "78. 2.1544346900319e-07\n",
       "79. 4.32876128108306e-07\n",
       "80. 8.69749002617787e-07\n",
       "81. 1.74752840000768e-06\n",
       "82. 3.51119173421514e-06\n",
       "83. 7.05480231071863e-06\n",
       "84. 1.41747416292681e-05\n",
       "85. 2.84803586843582e-05\n",
       "86. 5.72236765935022e-05\n",
       "87. 0.000114975699539774\n",
       "88. 0.000231012970008316\n",
       "89. 0.000464158883361279\n",
       "90. 0.000932603346883218\n",
       "91. 0.00187381742286039\n",
       "92. 0.00376493580679249\n",
       "93. 0.00756463327554629\n",
       "94. 0.0151991108295294\n",
       "95. 0.0305385550883341\n",
       "96. 0.0613590727341319\n",
       "97. 0.123284673944207\n",
       "98. 0.247707635599171\n",
       "99. 0.497702356433213\n",
       "100. 1\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  [1] 1.000000e-30 2.009233e-30 4.037017e-30 8.111308e-30 1.629751e-29\n",
       "  [6] 3.274549e-29 6.579332e-29 1.321941e-28 2.656088e-28 5.336699e-28\n",
       " [11] 1.072267e-27 2.154435e-27 4.328761e-27 8.697490e-27 1.747528e-26\n",
       " [16] 3.511192e-26 7.054802e-26 1.417474e-25 2.848036e-25 5.722368e-25\n",
       " [21] 1.149757e-24 2.310130e-24 4.641589e-24 9.326033e-24 1.873817e-23\n",
       " [26] 3.764936e-23 7.564633e-23 1.519911e-22 3.053856e-22 6.135907e-22\n",
       " [31] 1.232847e-21 2.477076e-21 4.977024e-21 1.000000e-20 2.009233e-20\n",
       " [36] 4.037017e-20 8.111308e-20 1.629751e-19 3.274549e-19 6.579332e-19\n",
       " [41] 1.321941e-18 2.656088e-18 5.336699e-18 1.072267e-17 2.154435e-17\n",
       " [46] 4.328761e-17 8.697490e-17 1.747528e-16 3.511192e-16 7.054802e-16\n",
       " [51] 1.417474e-15 2.848036e-15 5.722368e-15 1.149757e-14 2.310130e-14\n",
       " [56] 4.641589e-14 9.326033e-14 1.873817e-13 3.764936e-13 7.564633e-13\n",
       " [61] 1.519911e-12 3.053856e-12 6.135907e-12 1.232847e-11 2.477076e-11\n",
       " [66] 4.977024e-11 1.000000e-10 2.009233e-10 4.037017e-10 8.111308e-10\n",
       " [71] 1.629751e-09 3.274549e-09 6.579332e-09 1.321941e-08 2.656088e-08\n",
       " [76] 5.336699e-08 1.072267e-07 2.154435e-07 4.328761e-07 8.697490e-07\n",
       " [81] 1.747528e-06 3.511192e-06 7.054802e-06 1.417474e-05 2.848036e-05\n",
       " [86] 5.722368e-05 1.149757e-04 2.310130e-04 4.641589e-04 9.326033e-04\n",
       " [91] 1.873817e-03 3.764936e-03 7.564633e-03 1.519911e-02 3.053856e-02\n",
       " [96] 6.135907e-02 1.232847e-01 2.477076e-01 4.977024e-01 1.000000e+00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lambda = 10^seq(-30,0,length.out=100)\n",
    "lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.ridge = glmnet(x=X,y=y,family=\"gaussian\",alpha=0,lambda=lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:  glmnet(x = X, y = y, family = \"gaussian\", alpha = 0, lambda = lambda) \n",
       "\n",
       "     Df   %Dev  Lambda\n",
       "1   600  97.10 1.00000\n",
       "2   600  97.85 0.49770\n",
       "3   600  98.48 0.24770\n",
       "4   600  98.94 0.12330\n",
       "5   600  99.27 0.06136\n",
       "6   600  99.54 0.03054\n",
       "7   600  99.75 0.01520\n",
       "8   600  99.87 0.00756\n",
       "9   600  99.94 0.00377\n",
       "10  600  99.96 0.00187\n",
       "11  600  99.98 0.00093\n",
       "12  600  99.98 0.00046\n",
       "13  600  99.99 0.00023\n",
       "14  600  99.99 0.00012\n",
       "15  600  99.99 0.00006\n",
       "16  600  99.99 0.00003\n",
       "17  600  99.99 0.00001\n",
       "18  600  99.99 0.00001\n",
       "19  600  99.99 0.00000\n",
       "20  600  99.99 0.00000\n",
       "21  600  99.99 0.00000\n",
       "22  600  99.99 0.00000\n",
       "23  600  99.99 0.00000\n",
       "24  600  99.99 0.00000\n",
       "25  600  99.99 0.00000\n",
       "26  600 100.00 0.00000\n",
       "27  600 100.00 0.00000\n",
       "28  600 100.00 0.00000\n",
       "29  600 100.00 0.00000\n",
       "30  600 100.00 0.00000\n",
       "31  600 100.00 0.00000\n",
       "32  600 100.00 0.00000\n",
       "33  600 100.00 0.00000\n",
       "34  600 100.00 0.00000\n",
       "35  600 100.00 0.00000\n",
       "36  600 100.00 0.00000\n",
       "37  600 100.00 0.00000\n",
       "38  600 100.00 0.00000\n",
       "39  600 100.00 0.00000\n",
       "40  600 100.00 0.00000\n",
       "41  600 100.00 0.00000\n",
       "42  600 100.00 0.00000\n",
       "43  600 100.00 0.00000\n",
       "44  600 100.00 0.00000\n",
       "45  600 100.00 0.00000\n",
       "46  600 100.00 0.00000\n",
       "47  600 100.00 0.00000\n",
       "48  600 100.00 0.00000\n",
       "49  600 100.00 0.00000\n",
       "50  600 100.00 0.00000\n",
       "51  600 100.00 0.00000\n",
       "52  600 100.00 0.00000\n",
       "53  600 100.00 0.00000\n",
       "54  600 100.00 0.00000\n",
       "55  600 100.00 0.00000\n",
       "56  600 100.00 0.00000\n",
       "57  600 100.00 0.00000\n",
       "58  600 100.00 0.00000\n",
       "59  600 100.00 0.00000\n",
       "60  600 100.00 0.00000\n",
       "61  600 100.00 0.00000\n",
       "62  600 100.00 0.00000\n",
       "63  600 100.00 0.00000\n",
       "64  600 100.00 0.00000\n",
       "65  600 100.00 0.00000\n",
       "66  600 100.00 0.00000\n",
       "67  600 100.00 0.00000\n",
       "68  600 100.00 0.00000\n",
       "69  600 100.00 0.00000\n",
       "70  600 100.00 0.00000\n",
       "71  600 100.00 0.00000\n",
       "72  600 100.00 0.00000\n",
       "73  600 100.00 0.00000\n",
       "74  600 100.00 0.00000\n",
       "75  600 100.00 0.00000\n",
       "76  600 100.00 0.00000\n",
       "77  600 100.00 0.00000\n",
       "78  600 100.00 0.00000\n",
       "79  600 100.00 0.00000\n",
       "80  600 100.00 0.00000\n",
       "81  600 100.00 0.00000\n",
       "82  600 100.00 0.00000\n",
       "83  600 100.00 0.00000\n",
       "84  600 100.00 0.00000\n",
       "85  600 100.00 0.00000\n",
       "86  600 100.00 0.00000\n",
       "87  600 100.00 0.00000\n",
       "88  600 100.00 0.00000\n",
       "89  600 100.00 0.00000\n",
       "90  600 100.00 0.00000\n",
       "91  600 100.00 0.00000\n",
       "92  600 100.00 0.00000\n",
       "93  600 100.00 0.00000\n",
       "94  600 100.00 0.00000\n",
       "95  600 100.00 0.00000\n",
       "96  600 100.00 0.00000\n",
       "97  600 100.00 0.00000\n",
       "98  600 100.00 0.00000\n",
       "99  600 100.00 0.00000\n",
       "100 600 100.00 0.00000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fit.ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>600</li><li>100</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 600\n",
       "\\item 100\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 600\n",
       "2. 100\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 600 100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(fit.ridge$beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   [[ suppressing 100 column names 's0', 's1', 's2' ... ]]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6 x 100 sparse Matrix of class \"dgCMatrix\"\n",
       "                                                                     \n",
       "X1367566_at   -5.477430e-04 -0.0009098604 -0.0012936785 -0.0016887030\n",
       "X1367568_a_at -5.307465e-04 -0.0007900896 -0.0009256767 -0.0010288732\n",
       "X1367570_at   -4.500267e-04 -0.0005748670 -0.0006497034 -0.0008750083\n",
       "X1367584_at   -3.736444e-04 -0.0004511997 -0.0003873789 -0.0002730788\n",
       "X1367614_at   -4.432339e-04 -0.0007507625 -0.0009610941 -0.0010132036\n",
       "X1367647_at    8.341947e-05 -0.0003631833 -0.0009558483 -0.0016231911\n",
       "                                                                     \n",
       "X1367566_at   -2.005544e-03 -0.0022545998 -0.0024639935 -2.648885e-03\n",
       "X1367568_a_at -9.956142e-04 -0.0009329712 -0.0008759390 -8.410879e-04\n",
       "X1367570_at   -1.203588e-03 -0.0016953258 -0.0022171353 -2.663982e-03\n",
       "X1367584_at   -2.845339e-05  0.0002543778  0.0006399053  1.074249e-03\n",
       "X1367614_at   -8.773350e-04 -0.0006063980 -0.0002779945  1.777952e-05\n",
       "X1367647_at   -2.285891e-03 -0.0029991644 -0.0036682157 -4.190753e-03\n",
       "                                                                    \n",
       "X1367566_at   -0.0027981695 -0.002909570 -0.0029935109 -0.0030611352\n",
       "X1367568_a_at -0.0008376675 -0.000841671 -0.0008475300 -0.0008541747\n",
       "X1367570_at   -0.0030215036 -0.003250977 -0.0034026409 -0.0035166647\n",
       "X1367584_at    0.0015022400  0.001834604  0.0020900967  0.0023070175\n",
       "X1367614_at    0.0002710001  0.000420164  0.0005047522  0.0005587496\n",
       "X1367647_at   -0.0045746503 -0.004795414 -0.0049252576 -0.0050108564\n",
       "                                                                     \n",
       "X1367566_at   -0.0031013809 -0.0031205781 -0.0031330934 -0.0031450788\n",
       "X1367568_a_at -0.0008571751 -0.0008579145 -0.0008585042 -0.0008593820\n",
       "X1367570_at   -0.0035820194 -0.0036122321 -0.0036316056 -0.0036500638\n",
       "X1367584_at    0.0024427737  0.0025087641  0.0025521500  0.0025944680\n",
       "X1367614_at    0.0005842737  0.0005940328  0.0005995014  0.0006042185\n",
       "X1367647_at   -0.0050539871 -0.0050723420 -0.0050834739 -0.0050935095\n",
       "                                                                     \n",
       "X1367566_at   -0.0031562304 -0.0031664861 -0.0031758727 -0.0031844521\n",
       "X1367568_a_at -0.0008603298 -0.0008612241 -0.0008620140 -0.0008626859\n",
       "X1367570_at   -0.0036673846 -0.0036835552 -0.0036986527 -0.0037127793\n",
       "X1367584_at    0.0026352728  0.0026744450  0.0027120128  0.0027480640\n",
       "X1367614_at    0.0006082664  0.0006117143  0.0006146258  0.0006170545\n",
       "X1367647_at   -0.0051023424 -0.0051100250 -0.0051166629 -0.0051223740\n",
       "                                                                     \n",
       "X1367566_at   -0.0031922974 -0.0031994812 -0.0032060717 -0.0032121308\n",
       "X1367568_a_at -0.0008632439 -0.0008636993 -0.0008640656 -0.0008643565\n",
       "X1367570_at   -0.0037260359 -0.0037385131 -0.0037502889 -0.0037614304\n",
       "X1367584_at    0.0027827048  0.0028160418  0.0028481744  0.0028791927\n",
       "X1367614_at    0.0006190438  0.0006206292  0.0006218405  0.0006227033\n",
       "X1367647_at   -0.0051272720 -0.0051314606 -0.0051350318 -0.0051380662\n",
       "                                                                     \n",
       "X1367566_at   -0.0032177137 -0.0032228697 -0.0032276423 -0.0032320699\n",
       "X1367568_a_at -0.0008645846 -0.0008647613 -0.0008648966 -0.0008649997\n",
       "X1367570_at   -0.0037719949 -0.0037820319 -0.0037915847 -0.0038006910\n",
       "X1367584_at    0.0029091772  0.0029381997  0.0029663237  0.0029936061\n",
       "X1367614_at    0.0006232407  0.0006234733  0.0006234208  0.0006231014\n",
       "X1367647_at   -0.0051406342 -0.0051427969 -0.0051446080 -0.0051461141\n",
       "                                                                    \n",
       "X1367566_at   -0.0032361866 -0.0032400225 -0.0032436044 -0.003246956\n",
       "X1367568_a_at -0.0008650782 -0.0008651390 -0.0008651882 -0.000865231\n",
       "X1367570_at   -0.0038093844 -0.0038176950 -0.0038256496 -0.003833273\n",
       "X1367584_at    0.0030200976  0.0030458439  0.0030708858  0.003095261\n",
       "X1367614_at    0.0006225324  0.0006217303  0.0006207109  0.000619489\n",
       "X1367647_at   -0.0051473558 -0.0051483688 -0.0051491841 -0.005149829\n",
       "                                                                     \n",
       "X1367566_at   -0.0032500982 -0.0032530499 -0.0032558277 -0.0032584465\n",
       "X1367568_a_at -0.0008652718 -0.0008653145 -0.0008653624 -0.0008654180\n",
       "X1367570_at   -0.0038405858 -0.0038476096 -0.0038543622 -0.0038608605\n",
       "X1367584_at    0.0031190015  0.0031421393  0.0031647018  0.0031867144\n",
       "X1367614_at    0.0006180789  0.0006164940  0.0006147472  0.0006128505\n",
       "X1367647_at   -0.0051503271 -0.0051506995 -0.0051509642 -0.0051511371\n",
       "                                                                     \n",
       "X1367566_at   -0.0032609197 -0.0032632592 -0.0032654757 -0.0032675788\n",
       "X1367568_a_at -0.0008654837 -0.0008655612 -0.0008656518 -0.0008657566\n",
       "X1367570_at   -0.0038671198 -0.0038731545 -0.0038789775 -0.0038846010\n",
       "X1367584_at    0.0032082006  0.0032291819  0.0032496781  0.0032697078\n",
       "X1367614_at    0.0006108154  0.0006086527  0.0006063726  0.0006039848\n",
       "X1367647_at   -0.0051512321 -0.0051512615 -0.0051512356 -0.0051511637\n",
       "                                                                     \n",
       "X1367566_at   -0.0032695771 -0.0032714784 -0.0032732898 -0.0032750176\n",
       "X1367568_a_at -0.0008658762 -0.0008660109 -0.0008661610 -0.0008663263\n",
       "X1367570_at   -0.0038900363 -0.0038952935 -0.0039003824 -0.0039053119\n",
       "X1367584_at    0.0032892878  0.0033084341  0.0033271614  0.0033454837\n",
       "X1367614_at    0.0006014982  0.0005989215  0.0005962627  0.0005935292\n",
       "X1367647_at   -0.0051510540 -0.0051509132 -0.0051507474 -0.0051505619\n",
       "                                                                     \n",
       "X1367566_at   -0.0032766675 -0.0032782449 -0.0032797544 -0.0032812004\n",
       "X1367568_a_at -0.0008665065 -0.0008667013 -0.0008669100 -0.0008671319\n",
       "X1367570_at   -0.0039100902 -0.0039147251 -0.0039192237 -0.0039235927\n",
       "X1367584_at    0.0033634139  0.0033809640  0.0033981456  0.0034149694\n",
       "X1367614_at    0.0005907283  0.0005878665  0.0005849502  0.0005819852\n",
       "X1367647_at   -0.0051503613 -0.0051501492 -0.0051499292 -0.0051497041\n",
       "                                                                     \n",
       "X1367566_at   -0.0032825868 -0.0032839172 -0.0032851949 -0.0032864230\n",
       "X1367568_a_at -0.0008673664 -0.0008676125 -0.0008678695 -0.0008681362\n",
       "X1367570_at   -0.0039278383 -0.0039319663 -0.0039359821 -0.0039398908\n",
       "X1367584_at    0.0034314457  0.0034475839  0.0034633933  0.0034788826\n",
       "X1367614_at    0.0005789770  0.0005759308  0.0005728515  0.0005697436\n",
       "X1367647_at   -0.0051494763 -0.0051492479 -0.0051490207 -0.0051487961\n",
       "                                                                     \n",
       "X1367566_at   -0.0032876042 -0.0032887410 -0.0032898359 -0.0032908909\n",
       "X1367568_a_at -0.0008684119 -0.0008686955 -0.0008689861 -0.0008692828\n",
       "X1367570_at   -0.0039436971 -0.0039474054 -0.0039510198 -0.0039545443\n",
       "X1367584_at    0.0034940599  0.0035089332  0.0035235100  0.0035377974\n",
       "X1367614_at    0.0005666113  0.0005634588  0.0005602897  0.0005571075\n",
       "X1367647_at   -0.0051485754 -0.0051483595 -0.0051481494 -0.0051479456\n",
       "                                                                     \n",
       "X1367566_at   -0.0032919082 -0.0032928895 -0.0032938366 -0.0032947510\n",
       "X1367568_a_at -0.0008695845 -0.0008698904 -0.0008701995 -0.0008705111\n",
       "X1367570_at   -0.0039579823 -0.0039613374 -0.0039646127 -0.0039678112\n",
       "X1367584_at    0.0035518022  0.0035655312  0.0035789906  0.0035921865\n",
       "X1367614_at    0.0005539157  0.0005507172  0.0005475149  0.0005443116\n",
       "X1367647_at   -0.0051477487 -0.0051475592 -0.0051473773 -0.0051472032\n",
       "                                                                     \n",
       "X1367566_at   -0.0032956343 -0.0032964879 -0.0032973130 -0.0032981109\n",
       "X1367568_a_at -0.0008708243 -0.0008711382 -0.0008714522 -0.0008717656\n",
       "X1367570_at   -0.0039709356 -0.0039739888 -0.0039769730 -0.0039798908\n",
       "X1367584_at    0.0036051247  0.0036178110  0.0036302506  0.0036424490\n",
       "X1367614_at    0.0005411097  0.0005379117  0.0005347198  0.0005315360\n",
       "X1367647_at   -0.0051470371 -0.0051468791 -0.0051467292 -0.0051465873\n",
       "                                                                     \n",
       "X1367566_at   -0.0032988827 -0.0032996294 -0.0033003521 -0.0033010516\n",
       "X1367568_a_at -0.0008720776 -0.0008723876 -0.0008726951 -0.0008729995\n",
       "X1367570_at   -0.0039827442 -0.0039855354 -0.0039882663 -0.0039909388\n",
       "X1367584_at    0.0036544112  0.0036661421  0.0036776464  0.0036889289\n",
       "X1367614_at    0.0005283623  0.0005252004  0.0005220521  0.0005189188\n",
       "X1367647_at   -0.0051464534 -0.0051463275 -0.0051462093 -0.0051460987\n",
       "                                                                     \n",
       "X1367566_at   -0.0033017290 -0.0033023849 -0.0033030203 -0.0033036357\n",
       "X1367568_a_at -0.0008733003 -0.0008735970 -0.0008738892 -0.0008741765\n",
       "X1367570_at   -0.0039935547 -0.0039961155 -0.0039986230 -0.0040010786\n",
       "X1367584_at    0.0036999941  0.0037108462  0.0037214895  0.0037319283\n",
       "X1367614_at    0.0005158021  0.0005127034  0.0005096238  0.0005065645\n",
       "X1367647_at   -0.0051459956 -0.0051458997 -0.0051458110 -0.0051457291\n",
       "                                                                     \n",
       "X1367566_at   -0.0033042321 -0.0033048099 -0.0033053699 -0.0033059126\n",
       "X1367568_a_at -0.0008744585 -0.0008747350 -0.0008750056 -0.0008752701\n",
       "X1367570_at   -0.0040034837 -0.0040058398 -0.0040081480 -0.0040104096\n",
       "X1367584_at    0.0037421664  0.0037522078  0.0037620564  0.0037717159\n",
       "X1367614_at    0.0005035267  0.0005005113  0.0004975192  0.0004945512\n",
       "X1367647_at   -0.0051456539 -0.0051455852 -0.0051455227 -0.0051454663\n",
       "                                                                     \n",
       "X1367566_at   -0.0033064386 -0.0033069485 -0.0033074428 -0.0033079219\n",
       "X1367568_a_at -0.0008755283 -0.0008757799 -0.0008760248 -0.0008762629\n",
       "X1367570_at   -0.0040126258 -0.0040147977 -0.0040169264 -0.0040190130\n",
       "X1367584_at    0.0037811899  0.0037904820  0.0037995956  0.0038085342\n",
       "X1367614_at    0.0004916083  0.0004886910  0.0004858000  0.0004829360\n",
       "X1367647_at   -0.0051454158 -0.0051453709 -0.0051453314 -0.0051452971\n",
       "                                                                     \n",
       "X1367566_at   -0.0033083864 -0.0033088367 -0.0033092733 -0.0033096965\n",
       "X1367568_a_at -0.0008764941 -0.0008767182 -0.0008769351 -0.0008771449\n",
       "X1367570_at   -0.0040210584 -0.0040230635 -0.0040250294 -0.0040269568\n",
       "X1367584_at    0.0038173010  0.0038258994  0.0038343325  0.0038426034\n",
       "X1367614_at    0.0004800994  0.0004772908  0.0004745106  0.0004717592\n",
       "X1367647_at   -0.0051452679 -0.0051452435 -0.0051452237 -0.0051452084\n",
       "                                                                    \n",
       "X1367566_at   -0.0033101068 -0.0033105045 -0.003310890 -0.0033112636\n",
       "X1367568_a_at -0.0008773475 -0.0008775429 -0.000877731 -0.0008779119\n",
       "X1367570_at   -0.0040288466 -0.0040306997 -0.004032517 -0.0040342987\n",
       "X1367584_at    0.0038507152  0.0038586709  0.003866473  0.0038741255\n",
       "X1367614_at    0.0004690369  0.0004663441  0.000463681  0.0004610479\n",
       "X1367647_at   -0.0051451974 -0.0051451905 -0.005145187 -0.0051451883\n",
       "                                                                     \n",
       "X1367566_at   -0.0033116258 -0.0033119768 -0.0033123169 -0.0033126465\n",
       "X1367568_a_at -0.0008780857 -0.0008782523 -0.0008784118 -0.0008785643\n",
       "X1367570_at   -0.0040360462 -0.0040377598 -0.0040394405 -0.0040410887\n",
       "X1367584_at    0.0038816301  0.0038889900  0.0038962078  0.0039032862\n",
       "X1367614_at    0.0004584449  0.0004558722  0.0004533300  0.0004508184\n",
       "X1367647_at   -0.0051451927 -0.0051452005 -0.0051452117 -0.0051452260\n",
       "                                                                     \n",
       "X1367566_at   -0.0033129658 -0.0033132751 -0.0033135748 -0.0033138650\n",
       "X1367568_a_at -0.0008787098 -0.0008788485 -0.0008789804 -0.0008791056\n",
       "X1367570_at   -0.0040427052 -0.0040442907 -0.0040458457 -0.0040473708\n",
       "X1367584_at    0.0039102279  0.0039170353  0.0039237109  0.0039302574\n",
       "X1367614_at    0.0004483374  0.0004458871  0.0004434675  0.0004410787\n",
       "X1367647_at   -0.0051452434 -0.0051452637 -0.0051452868 -0.0051453125\n",
       "                                                                     \n",
       "X1367566_at   -0.0033141461 -0.0033144182 -0.0033146816 -0.0033149366\n",
       "X1367568_a_at -0.0008792242 -0.0008793364 -0.0008794422 -0.0008795417\n",
       "X1367570_at   -0.0040488666 -0.0040503337 -0.0040517727 -0.0040531841\n",
       "X1367584_at    0.0039366769  0.0039429721  0.0039491450  0.0039551981\n",
       "X1367614_at    0.0004387205  0.0004363931  0.0004340963  0.0004318300\n",
       "X1367647_at   -0.0051453408 -0.0051453715 -0.0051454045 -0.0051454397"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(fit.ridge$beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "100"
      ],
      "text/latex": [
       "100"
      ],
      "text/markdown": [
       "100"
      ],
      "text/plain": [
       "[1] 100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "length(fit.ridge$lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAP1BMVEUAAAAil+Yo4uVNTU1h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\nSUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplot(log(fit.ridge$lambda),t(fit.ridge$beta),type='l',xlab=\"log(lam)\",ylab=\"beta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "\"collapsing to unique 'x' values\"\n",
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "\"collapsing to unique 'x' values\"\n",
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "\"collapsing to unique 'x' values\"\n",
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "\"collapsing to unique 'x' values\"\n",
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "\"collapsing to unique 'x' values\"\n",
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "\"collapsing to unique 'x' values\"\n",
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "\"collapsing to unique 'x' values\"\n",
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "\"collapsing to unique 'x' values\"\n",
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "\"collapsing to unique 'x' values\"\n",
      "Warning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n",
      "\"collapsing to unique 'x' values\"\n"
     ]
    }
   ],
   "source": [
    "cv.ridge = cv.glmnet(x=X,y=y,family=\"gaussian\",alpha=0,lambda=lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAANlBMVEUAAABNTU1oaGh8fHyM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\nCBKggCABCv4DoDfHymGRC3IAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(cv.ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "4.32876128108306e-17"
      ],
      "text/latex": [
       "4.32876128108306e-17"
      ],
      "text/markdown": [
       "4.32876128108306e-17"
      ],
      "text/plain": [
       "[1] 4.328761e-17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv.ridge$lambda.min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.00187381742286039"
      ],
      "text/latex": [
       "0.00187381742286039"
      ],
      "text/markdown": [
       "0.00187381742286039"
      ],
      "text/plain": [
       "[1] 0.001873817"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv.ridge$lambda.1se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "best.ridge = glmnet(x=X,y=y,family=\"gaussian\",alpha=0,lambda=cv.ridge$lambda.min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:  glmnet(x = X, y = y, family = \"gaussian\", alpha = 0, lambda = cv.ridge$lambda.min) \n",
       "\n",
       "   Df  %Dev    Lambda\n",
       "1 600 99.99 4.329e-17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best.ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6 x 1 sparse Matrix of class \"dgCMatrix\"\n",
       "                      s0\n",
       "X1398514_at -0.001163058\n",
       "X1398577_at -0.004809884\n",
       "X1398578_at  0.003526449\n",
       "X1398625_at  0.003532700\n",
       "X1398634_at  0.005280243\n",
       "X1398716_at -0.011496051"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tail(coef(best.ridge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = predict(best.ridge,newx=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nIEBIECAkCBASBAgJAoQEAUKCgP8BfhxaAynpcdIAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(y,y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,Rmd,R"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
